{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OQzsLd9LqRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWIFsSvaK1dS",
        "outputId": "67cd82dd-24b5-405e-c0d6-8312d8abdabd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should print True if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should print True if GPU is available"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXTtPX87LjC7",
        "outputId": "ea9b8e8e-f7a5-42f3-cdae-709e9baebcc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python diffusers torch torchvision transformers accelerate numpy pillow opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6_IYgsZLsMy",
        "outputId": "bbe10530-b672-4e7b-c881-b9fab5f4d6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp311-cp311-linux_x86_64.whl size=5959635 sha256=3de07cd2811eeb73ac60db5494d5724775418735c1edad8d9ef914e677067f7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/03/66/eb3810eafd55d921b2be32896d1f44313996982360663aa80b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, diskcache, nvidia-cusparse-cu12, nvidia-cudnn-cu12, llama-cpp-python, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.8 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DIBDkAcLyVH",
        "outputId": "c6f3d2ca-7175-46af-b695-86f2a43b9a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.25.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.25.2-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.25.2 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuruPvppND8X",
        "outputId": "d52700bb-3a51-487b-9f9f-54217ca76b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python torch diffusers transformers accelerate safetensors matplotlib Pillow gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y5MJKsdNNf4",
        "outputId": "7403ef14-a499-479e-d357-cb3497e5f3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.25.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from llama_cpp import Llama\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# Load the Llama Model\n",
        "def load_llama_model(model_path=None):\n",
        "    # If no model path provided, use default path\n",
        "    if not model_path:\n",
        "        model_path = \"/content/drive/MyDrive/llama-2-7b.Q3_K_S.gguf\"\n",
        "\n",
        "    # Check if model exists at the specified path\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Warning: Model not found at {model_path}\")\n",
        "        print(\"You may need to adjust the model path or download the model first.\")\n",
        "        raise FileNotFoundError(f\"Model not found at {model_path}\")\n",
        "\n",
        "    model = Llama(\n",
        "        model_path=model_path,\n",
        "        n_ctx=2048,\n",
        "        verbose=False  # Reduce the logging output\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Enhanced story generation function\n",
        "def generate_story(model, prompt, max_tokens=500):\n",
        "    # Create a more specific prompt that encourages creative storytelling\n",
        "    story_prompt = f\"\"\"<s>[INST] Write an original, engaging short story based on the following prompt:\n",
        "\n",
        "    \"{prompt}\"\n",
        "\n",
        "    Make it creative with a clear beginning, middle, and end. Include interesting characters,\n",
        "    dialogue, and descriptive language. The story should be approximately 200-300 words. [/INST]\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Increase temperature for more creative outputs\n",
        "        response = model(\n",
        "            story_prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=0.85,    # Slightly higher temperature for creativity\n",
        "            top_p=0.92,          # Slightly higher top_p for more varied outputs\n",
        "            frequency_penalty=0.3,  # Reduce repetition\n",
        "            presence_penalty=0.3,   # Encourage novel wording\n",
        "            echo=False,\n",
        "            stop=[\"</s>\", \"[INST]\"]  # Stop at proper ending tokens\n",
        "        )\n",
        "\n",
        "        generated_text = response['choices'][0]['text'].strip()\n",
        "\n",
        "        # Only fall back if the response is truly problematic\n",
        "        if len(generated_text) < 50 or \"[INST]\" in generated_text:\n",
        "            print(\"Warning: Low quality story generated. Retrying with different parameters...\")\n",
        "\n",
        "            # Second attempt with different parameters\n",
        "            retry_response = model(\n",
        "                story_prompt,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=0.75,  # More moderate temperature\n",
        "                top_p=0.85,        # More focused sampling\n",
        "                frequency_penalty=0.2,\n",
        "                presence_penalty=0.2,\n",
        "                echo=False,\n",
        "                stop=[\"</s>\", \"[INST]\"]\n",
        "            )\n",
        "\n",
        "            retry_text = retry_response['choices'][0]['text'].strip()\n",
        "\n",
        "            if len(retry_text) >= 50 and \"[INST]\" not in retry_text:\n",
        "                return retry_text\n",
        "            else:\n",
        "                # Only use fallback as a last resort\n",
        "                print(\"Warning: Retry failed. Creating a dynamic fallback story.\")\n",
        "                # Create a more dynamic fallback story based on the prompt\n",
        "                return create_dynamic_fallback_story(prompt)\n",
        "\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in story generation: {e}\")\n",
        "        return create_dynamic_fallback_story(prompt)\n",
        "\n",
        "# Function to create a more varied fallback story when needed\n",
        "def create_dynamic_fallback_story(prompt):\n",
        "    # Extract potential characters or themes from the prompt\n",
        "    words = prompt.lower().split()\n",
        "\n",
        "    # Look for keywords that might be character types or settings\n",
        "    character_keywords = [\"about\", \"with\", \"featuring\", \"starring\", \"where\"]\n",
        "    setting_keywords = [\"in\", \"at\", \"during\", \"through\"]\n",
        "\n",
        "    characters = \"characters\"\n",
        "    setting = \"a mysterious place\"\n",
        "\n",
        "    # Extract potential character information\n",
        "    for keyword in character_keywords:\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            if idx + 1 < len(words):\n",
        "                characters = \" \".join(words[idx+1:min(idx+6, len(words))])\n",
        "                break\n",
        "\n",
        "    # Extract potential setting information\n",
        "    for keyword in setting_keywords:\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            if idx + 1 < len(words):\n",
        "                setting = \" \".join(words[idx+1:min(idx+4, len(words))])\n",
        "                break\n",
        "\n",
        "    # Create one of several story templates using the extracted information\n",
        "    import random\n",
        "    templates = [\n",
        "        f\"The sun cast long shadows across {setting} as {characters} embarked on an unexpected journey. \"\n",
        "        f\"What began as an ordinary day quickly transformed when they discovered a hidden secret that would change everything. \"\n",
        "        f\"Through challenges and revelations, they found strength in each other they never knew existed. \"\n",
        "        f\"By the journey's end, nothing would ever be the same for them again.\",\n",
        "\n",
        "        f\"A chance encounter in {setting} brought {characters} together at precisely the right moment. \"\n",
        "        f\"Neither could have anticipated how their lives would intertwine, nor the adventure that awaited them. \"\n",
        "        f\"Facing impossible odds, they were forced to trust each other despite their differences. \"\n",
        "        f\"Their story became one that would be told for generations to come.\",\n",
        "\n",
        "        f\"Deep within {setting}, {characters} discovered something that should have remained hidden. \"\n",
        "        f\"As they unraveled ancient mysteries, tension grew between them, testing the limits of their relationship. \"\n",
        "        f\"A moment of truth revealed their true character when everything was at stake. \"\n",
        "        f\"In the end, they emerged transformed, carrying both scars and wisdom from their extraordinary experience.\"\n",
        "    ]\n",
        "\n",
        "    # Return a randomly selected template\n",
        "    return random.choice(templates)\n",
        "\n",
        "# Function to split the story into panels\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    # First check if we have enough content to split\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    # Try to split by paragraphs first for more natural divisions\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]  # Remove empty paragraphs\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        # We have enough paragraphs, so use them as natural divisions\n",
        "        if len(paragraphs) > num_panels:\n",
        "            # Combine paragraphs to match the desired number of panels\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "            return panels\n",
        "        else:\n",
        "            # Perfect match or fewer paragraphs than panels\n",
        "            return paragraphs[:num_panels]\n",
        "\n",
        "    # If we don't have enough paragraphs, fall back to sentence splitting\n",
        "    sentences = []\n",
        "    # Split by common sentence endings\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]  # Remove empty or too-short sentences\n",
        "\n",
        "    # Handle edge case where there are fewer sentences than panels\n",
        "    if len(sentences) < num_panels:\n",
        "        # Create more sentences by splitting the longest ones\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            # Split at a comma or other natural break point if possible\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                # If no good split point, duplicate the sentence with a variation\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    # Calculate how many sentences per panel for a balanced distribution\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        # For the last panel, include any remaining sentences\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "    return panels\n",
        "\n",
        "# Load the Stable Diffusion XL model\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available, otherwise use CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Function to generate comic panels from any story prompt\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\"):\n",
        "    # Make sure the story prompt is compelling\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        # Load the Llama model\n",
        "        print(\"Loading Llama model...\")\n",
        "        llama_model = load_llama_model(model_path)\n",
        "\n",
        "        # Generate story from prompt with multiple attempts if needed\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(llama_model, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # Check if story generated properly\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Trying one more time with different approach.\")\n",
        "            # Try a more direct approach with the model\n",
        "            direct_prompt = f\"\"\"<s>[INST] Write a creative short story for children about {story_prompt}. Make it exciting with interesting characters and events. [/INST]\"\"\"\n",
        "\n",
        "            retry_response = llama_model(\n",
        "                direct_prompt,\n",
        "                max_tokens=600,\n",
        "                temperature=0.8,\n",
        "                top_p=0.9,\n",
        "                echo=False,\n",
        "                stop=[\"</s>\", \"[INST]\"]\n",
        "            )\n",
        "\n",
        "            story_text = retry_response['choices'][0]['text'].strip()\n",
        "\n",
        "            if len(story_text) < 50:\n",
        "                print(\"Still unable to generate a good story. Using the last fallback option.\")\n",
        "                story_text = (\n",
        "                    f\"In a world where anything was possible, {story_prompt} became reality. \"\n",
        "                    f\"The adventure began unexpectedly one morning when strange events unfolded. \"\n",
        "                    f\"Characters found themselves facing mysterious challenges that required courage and wit. \"\n",
        "                    f\"After many twists and turns, they discovered a solution that brought everything to a satisfying conclusion.\"\n",
        "                )\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating story: {e}\")\n",
        "        # Provide a fallback story\n",
        "        story_text = (\n",
        "            f\"In a world where anything was possible, {story_prompt} became reality. \"\n",
        "            f\"The adventure began unexpectedly one morning when strange events unfolded. \"\n",
        "            f\"Characters found themselves facing mysterious challenges that required courage and wit. \"\n",
        "            f\"After many twists and turns, they discovered a solution that brought everything to a satisfying conclusion.\"\n",
        "        )\n",
        "\n",
        "    # Split story into panels\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "\n",
        "    try:\n",
        "        # Load SDXL model\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "        for i, panel_text in enumerate(story_panels):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Create a prompt that helps SDXL understand it's a comic panel\n",
        "            # Use the art style specified by the user\n",
        "            prompt = f\"A detailed {art_style} illustration for a comic panel showing: {panel_text} cinematic lighting, expressive characters, vibrant colors\"\n",
        "\n",
        "            # Generate the image\n",
        "            image = pipe(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "            panel_images.append(image)\n",
        "\n",
        "            # Save the image\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Comic generation complete! All panels saved as PNG files.\")\n",
        "        return story_text, story_panels, panel_images\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a unique story and convert it into comic panels\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Write a story about a brave cat who learns to fly\",\n",
        "                    lines=2\n",
        "                )\n",
        "                num_panels = gr.Slider(minimum=2, maximum=8, value=4, step=1, label=\"Number of Panels\")\n",
        "                model_path = gr.Textbox(label=\"Optional: Custom Llama Model Path\", placeholder=\"Leave empty for default\")\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\n",
        "                        \"cartoon style\",\n",
        "                        \"manga style\",\n",
        "                        \"comic book style\",\n",
        "                        \"watercolor style\",\n",
        "                        \"3D rendered\",\n",
        "                        \"pixel art style\",\n",
        "                        \"oil painting style\",\n",
        "                        \"storybook illustration style\"\n",
        "                    ],\n",
        "                    value=\"cartoon style\",\n",
        "                    label=\"Art Style\"\n",
        "                )\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                story_output = gr.Textbox(label=\"Generated Story\", lines=8)\n",
        "\n",
        "        # Fixed Gallery component\n",
        "        panel_gallery = gr.Gallery(label=\"Comic Panels\", columns=2, rows=2, height=600)\n",
        "\n",
        "        # Status output for error messages\n",
        "        status_output = gr.Textbox(label=\"Status\", lines=2)\n",
        "\n",
        "        def run_generation(prompt, panels, path, style):\n",
        "            if not prompt:\n",
        "                return \"Please enter a story prompt\", [], \"Error: No prompt provided\"\n",
        "\n",
        "            model_path_to_use = path if path.strip() else None\n",
        "\n",
        "            try:\n",
        "                story_text, story_panels, images = generate_comic(prompt, int(panels), model_path_to_use, style)\n",
        "\n",
        "                if story_text and images:\n",
        "                    status = \"Success: Comic generated successfully!\"\n",
        "                elif story_text:\n",
        "                    status = \"Warning: Story generated but image creation failed\"\n",
        "                else:\n",
        "                    status = \"Error: Story generation failed\"\n",
        "\n",
        "                return story_text, images if images else [], status\n",
        "            except Exception as e:\n",
        "                return f\"An error occurred: {str(e)}\", [], f\"Error: {str(e)}\"\n",
        "\n",
        "        generate_button.click(\n",
        "            run_generation,\n",
        "            inputs=[story_prompt, num_panels, model_path, art_style],\n",
        "            outputs=[story_output, panel_gallery, status_output]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Main function to either run the generator directly or launch the web UI\n",
        "def main(use_web_ui=True):\n",
        "    if use_web_ui:\n",
        "        app = create_ui()\n",
        "        app.launch(share=True)  # Enable sharing for remote access\n",
        "    else:\n",
        "        # Example without web UI\n",
        "        story_prompt = input(\"Enter a story prompt: \")\n",
        "        num_panels = int(input(\"Enter number of panels (2-8): \"))\n",
        "        art_style = input(\"Enter art style (e.g., cartoon style, manga style): \")\n",
        "\n",
        "        story, panels, images = generate_comic(story_prompt, num_panels, None, art_style)\n",
        "\n",
        "        if images:\n",
        "            for i, img in enumerate(images):\n",
        "                plt.figure(figsize=(8, 8))\n",
        "                plt.imshow(img)\n",
        "                plt.title(f\"Panel {i+1}\")\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(use_web_ui=True)  # Set to False to use command line instead of web UI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "KaVq_gs1PEl6",
        "outputId": "f45db2cc-42d7-41fd-e43f-745953766366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/04/23 07:03:22 [W] [service.go:132] login to server failed: dial tcp 44.237.78.176:7000: i/o timeout\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from llama_cpp import Llama\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# Load the Llama Model\n",
        "def load_llama_model(model_path=None):\n",
        "    # If no model path provided, use default path\n",
        "    if not model_path:\n",
        "        model_path = \"/content/drive/MyDrive/llama-2-7b.Q3_K_S.gguf\"\n",
        "\n",
        "    # Check if model exists at the specified path\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Warning: Model not found at {model_path}\")\n",
        "        print(\"You may need to adjust the model path or download the model first.\")\n",
        "        raise FileNotFoundError(f\"Model not found at {model_path}\")\n",
        "\n",
        "    model = Llama(\n",
        "        model_path=model_path,\n",
        "        n_ctx=2048,\n",
        "        verbose=False  # Reduce the logging output\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Enhanced story generation function\n",
        "def generate_story(model, prompt, max_tokens=500):\n",
        "    # Create a more specific prompt that encourages creative storytelling\n",
        "    story_prompt = f\"\"\"<s>[INST] Write an original, engaging short story based on the following prompt:\n",
        "\n",
        "    \"{prompt}\"\n",
        "\n",
        "    Make it creative with a clear beginning, middle, and end. Include interesting characters,\n",
        "    dialogue, and descriptive language. The story should be approximately 200-300 words. [/INST]\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Increase temperature for more creative outputs\n",
        "        response = model(\n",
        "            story_prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=0.85,    # Slightly higher temperature for creativity\n",
        "            top_p=0.92,          # Slightly higher top_p for more varied outputs\n",
        "            frequency_penalty=0.3,  # Reduce repetition\n",
        "            presence_penalty=0.3,   # Encourage novel wording\n",
        "            echo=False,\n",
        "            stop=[\"</s>\", \"[INST]\"]  # Stop at proper ending tokens\n",
        "        )\n",
        "\n",
        "        generated_text = response['choices'][0]['text'].strip()\n",
        "\n",
        "        # Only fall back if the response is truly problematic\n",
        "        if len(generated_text) < 50 or \"[INST]\" in generated_text:\n",
        "            print(\"Warning: Low quality story generated. Retrying with different parameters...\")\n",
        "\n",
        "            # Second attempt with different parameters\n",
        "            retry_response = model(\n",
        "                story_prompt,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=0.75,  # More moderate temperature\n",
        "                top_p=0.85,        # More focused sampling\n",
        "                frequency_penalty=0.2,\n",
        "                presence_penalty=0.2,\n",
        "                echo=False,\n",
        "                stop=[\"</s>\", \"[INST]\"]\n",
        "            )\n",
        "\n",
        "            retry_text = retry_response['choices'][0]['text'].strip()\n",
        "\n",
        "            if len(retry_text) >= 50 and \"[INST]\" not in retry_text:\n",
        "                return retry_text\n",
        "            else:\n",
        "                # Only use fallback as a last resort\n",
        "                print(\"Warning: Retry failed. Creating a dynamic fallback story.\")\n",
        "                # Create a more dynamic fallback story based on the prompt\n",
        "                return create_dynamic_fallback_story(prompt)\n",
        "\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in story generation: {e}\")\n",
        "        return create_dynamic_fallback_story(prompt)\n",
        "\n",
        "# Function to create a more varied fallback story when needed\n",
        "def create_dynamic_fallback_story(prompt):\n",
        "    # Extract potential characters or themes from the prompt\n",
        "    words = prompt.lower().split()\n",
        "\n",
        "    # Look for keywords that might be character types or settings\n",
        "    character_keywords = [\"about\", \"with\", \"featuring\", \"starring\", \"where\"]\n",
        "    setting_keywords = [\"in\", \"at\", \"during\", \"through\"]\n",
        "\n",
        "    characters = \"characters\"\n",
        "    setting = \"a mysterious place\"\n",
        "\n",
        "    # Extract potential character information\n",
        "    for keyword in character_keywords:\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            if idx + 1 < len(words):\n",
        "                characters = \" \".join(words[idx+1:min(idx+6, len(words))])\n",
        "                break\n",
        "\n",
        "    # Extract potential setting information\n",
        "    for keyword in setting_keywords:\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            if idx + 1 < len(words):\n",
        "                setting = \" \".join(words[idx+1:min(idx+4, len(words))])\n",
        "                break\n",
        "\n",
        "    # Create one of several story templates using the extracted information\n",
        "    import random\n",
        "    templates = [\n",
        "        f\"The sun cast long shadows across {setting} as {characters} embarked on an unexpected journey. \"\n",
        "        f\"What began as an ordinary day quickly transformed when they discovered a hidden secret that would change everything. \"\n",
        "        f\"Through challenges and revelations, they found strength in each other they never knew existed. \"\n",
        "        f\"By the journey's end, nothing would ever be the same for them again.\",\n",
        "\n",
        "        f\"A chance encounter in {setting} brought {characters} together at precisely the right moment. \"\n",
        "        f\"Neither could have anticipated how their lives would intertwine, nor the adventure that awaited them. \"\n",
        "        f\"Facing impossible odds, they were forced to trust each other despite their differences. \"\n",
        "        f\"Their story became one that would be told for generations to come.\",\n",
        "\n",
        "        f\"Deep within {setting}, {characters} discovered something that should have remained hidden. \"\n",
        "        f\"As they unraveled ancient mysteries, tension grew between them, testing the limits of their relationship. \"\n",
        "        f\"A moment of truth revealed their true character when everything was at stake. \"\n",
        "        f\"In the end, they emerged transformed, carrying both scars and wisdom from their extraordinary experience.\"\n",
        "    ]\n",
        "\n",
        "    # Return a randomly selected template\n",
        "    return random.choice(templates)\n",
        "\n",
        "# Function to split the story into panels\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    # First check if we have enough content to split\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    # Try to split by paragraphs first for more natural divisions\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]  # Remove empty paragraphs\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        # We have enough paragraphs, so use them as natural divisions\n",
        "        if len(paragraphs) > num_panels:\n",
        "            # Combine paragraphs to match the desired number of panels\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "            return panels\n",
        "        else:\n",
        "            # Perfect match or fewer paragraphs than panels\n",
        "            return paragraphs[:num_panels]\n",
        "\n",
        "    # If we don't have enough paragraphs, fall back to sentence splitting\n",
        "    sentences = []\n",
        "    # Split by common sentence endings\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]  # Remove empty or too-short sentences\n",
        "\n",
        "    # Handle edge case where there are fewer sentences than panels\n",
        "    if len(sentences) < num_panels:\n",
        "        # Create more sentences by splitting the longest ones\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            # Split at a comma or other natural break point if possible\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                # If no good split point, duplicate the sentence with a variation\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    # Calculate how many sentences per panel for a balanced distribution\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        # For the last panel, include any remaining sentences\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "    return panels\n",
        "\n",
        "# Load the Stable Diffusion XL model\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available, otherwise use CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Function to generate comic panels from any story prompt\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\"):\n",
        "    # Make sure the story prompt is compelling\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        # Load the Llama model\n",
        "        print(\"Loading Llama model...\")\n",
        "        llama_model = load_llama_model(model_path)\n",
        "\n",
        "        # Generate story from prompt with multiple attempts if needed\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(llama_model, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # Check if story generated properly\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Trying one more time with different approach.\")\n",
        "            # Try a more direct approach with the model\n",
        "            direct_prompt = f\"\"\"<s>[INST] Write a creative short story for children about {story_prompt}. Make it exciting with interesting characters and events. [/INST]\"\"\"\n",
        "\n",
        "            retry_response = llama_model(\n",
        "                direct_prompt,\n",
        "                max_tokens=600,\n",
        "                temperature=0.8,\n",
        "                top_p=0.9,\n",
        "                echo=False,\n",
        "                stop=[\"</s>\", \"[INST]\"]\n",
        "            )\n",
        "\n",
        "            story_text = retry_response['choices'][0]['text'].strip()\n",
        "\n",
        "            if len(story_text) < 50:\n",
        "                print(\"Still unable to generate a good story. Using the last fallback option.\")\n",
        "                story_text = (\n",
        "                    f\"In a world where anything was possible, {story_prompt} became reality. \"\n",
        "                    f\"The adventure began unexpectedly one morning when strange events unfolded. \"\n",
        "                    f\"Characters found themselves facing mysterious challenges that required courage and wit. \"\n",
        "                    f\"After many twists and turns, they discovered a solution that brought everything to a satisfying conclusion.\"\n",
        "                )\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating story: {e}\")\n",
        "        # Provide a fallback story\n",
        "        story_text = (\n",
        "            f\"In a world where anything was possible, {story_prompt} became reality. \"\n",
        "            f\"The adventure began unexpectedly one morning when strange events unfolded. \"\n",
        "            f\"Characters found themselves facing mysterious challenges that required courage and wit. \"\n",
        "            f\"After many twists and turns, they discovered a solution that brought everything to a satisfying conclusion.\"\n",
        "        )\n",
        "\n",
        "    # Split story into panels\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "\n",
        "    try:\n",
        "        # Load SDXL model\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "        for i, panel_text in enumerate(story_panels):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Create a prompt that helps SDXL understand it's a comic panel\n",
        "            # Use the art style specified by the user\n",
        "            prompt = f\"A detailed {art_style} illustration for a comic panel showing: {panel_text} cinematic lighting, expressive characters, vibrant colors\"\n",
        "\n",
        "            # Generate the image\n",
        "            image = pipe(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "            panel_images.append(image)\n",
        "\n",
        "            # Save the image\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Comic generation complete! All panels saved as PNG files.\")\n",
        "        return story_text, story_panels, panel_images\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a unique story and convert it into comic panels\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Write a story about a brave cat who learns to fly\",\n",
        "                    lines=2\n",
        "                )\n",
        "                num_panels = gr.Slider(minimum=2, maximum=8, value=4, step=1, label=\"Number of Panels\")\n",
        "                model_path = gr.Textbox(label=\"Optional: Custom Llama Model Path\", placeholder=\"Leave empty for default\")\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\n",
        "                        \"cartoon style\",\n",
        "                        \"manga style\",\n",
        "                        \"comic book style\",\n",
        "                        \"watercolor style\",\n",
        "                        \"3D rendered\",\n",
        "                        \"pixel art style\",\n",
        "                        \"oil painting style\",\n",
        "                        \"storybook illustration style\"\n",
        "                    ],\n",
        "                    value=\"cartoon style\",\n",
        "                    label=\"Art Style\"\n",
        "                )\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                story_output = gr.Textbox(label=\"Generated Story\", lines=8)\n",
        "\n",
        "        # Fixed Gallery component\n",
        "        panel_gallery = gr.Gallery(label=\"Comic Panels\", columns=2, rows=2, height=600)\n",
        "\n",
        "        # Status output for error messages\n",
        "        status_output = gr.Textbox(label=\"Status\", lines=2)\n",
        "\n",
        "        def run_generation(prompt, panels, path, style):\n",
        "            if not prompt:\n",
        "                return \"Please enter a story prompt\", [], \"Error: No prompt provided\"\n",
        "\n",
        "            model_path_to_use = path if path.strip() else None\n",
        "\n",
        "            try:\n",
        "                story_text, story_panels, images = generate_comic(prompt, int(panels), model_path_to_use, style)\n",
        "\n",
        "                if story_text and images:\n",
        "                    status = \"Success: Comic generated successfully!\"\n",
        "                elif story_text:\n",
        "                    status = \"Warning: Story generated but image creation failed\"\n",
        "                else:\n",
        "                    status = \"Error: Story generation failed\"\n",
        "\n",
        "                return story_text, images if images else [], status\n",
        "            except Exception as e:\n",
        "                return f\"An error occurred: {str(e)}\", [], f\"Error: {str(e)}\"\n",
        "\n",
        "        generate_button.click(\n",
        "            run_generation,\n",
        "            inputs=[story_prompt, num_panels, model_path, art_style],\n",
        "            outputs=[story_output, panel_gallery, status_output]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Main function to either run the generator directly or launch the web UI\n",
        "def main(use_web_ui=True):\n",
        "    if use_web_ui:\n",
        "        app = create_ui()\n",
        "        app.launch(share=True)  # Enable sharing for remote access\n",
        "    else:\n",
        "        # Example without web UI\n",
        "        story_prompt = input(\"Enter a story prompt: \")\n",
        "        num_panels = int(input(\"Enter number of panels (2-8): \"))\n",
        "        art_style = input(\"Enter art style (e.g., cartoon style, manga style): \")\n",
        "\n",
        "        story, panels, images = generate_comic(story_prompt, num_panels, None, art_style)\n",
        "\n",
        "        if images:\n",
        "            for i, img in enumerate(images):\n",
        "                plt.figure(figsize=(8, 8))\n",
        "                plt.imshow(img)\n",
        "                plt.title(f\"Panel {i+1}\")\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(use_web_ui=True)  # Set to False to use command line instead of web UI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "A8k40rOWPMJd",
        "outputId": "d8c0d6ce-2d27-404d-e010-a19f6db003a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5a730fe9ca58cee57a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5a730fe9ca58cee57a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from llama_cpp import Llama\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# Load the Llama Model\n",
        "def load_llama_model(model_path=None):\n",
        "    # If no model path provided, use default path\n",
        "    if not model_path:\n",
        "        model_path = \"/content/drive/MyDrive/llama-2-7b.Q3_K_S.gguf\"\n",
        "\n",
        "    # Check if model exists at the specified path\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Warning: Model not found at {model_path}\")\n",
        "        print(\"You may need to adjust the model path or download the model first.\")\n",
        "        raise FileNotFoundError(f\"Model not found at {model_path}\")\n",
        "\n",
        "    model = Llama(\n",
        "        model_path=model_path,\n",
        "        n_ctx=2048,\n",
        "        verbose=False  # Reduce the logging output\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Enhanced story generation function\n",
        "def generate_story(model, prompt, max_tokens=500):\n",
        "    # Create a more specific prompt that encourages creative storytelling\n",
        "    story_prompt = f\"\"\"<s>[INST] Write an original, engaging short story based on the following prompt:\n",
        "\n",
        "    \"{prompt}\"\n",
        "\n",
        "    Make it creative with a clear beginning, middle, and end. Include interesting characters,\n",
        "    dialogue, and descriptive language. The story should be approximately 200-300 words. [/INST]\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Increase temperature for more creative outputs\n",
        "        response = model(\n",
        "            story_prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=0.85,    # Slightly higher temperature for creativity\n",
        "            top_p=0.92,          # Slightly higher top_p for more varied outputs\n",
        "            frequency_penalty=0.3,  # Reduce repetition\n",
        "            presence_penalty=0.3,   # Encourage novel wording\n",
        "            echo=False,\n",
        "            stop=[\"</s>\", \"[INST]\"]  # Stop at proper ending tokens\n",
        "        )\n",
        "\n",
        "        generated_text = response['choices'][0]['text'].strip()\n",
        "\n",
        "        # Only fall back if the response is truly problematic\n",
        "        if len(generated_text) < 50 or \"[INST]\" in generated_text:\n",
        "            print(\"Warning: Low quality story generated. Retrying with different parameters...\")\n",
        "\n",
        "            # Second attempt with different parameters\n",
        "            retry_response = model(\n",
        "                story_prompt,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=0.75,  # More moderate temperature\n",
        "                top_p=0.85,        # More focused sampling\n",
        "                frequency_penalty=0.2,\n",
        "                presence_penalty=0.2,\n",
        "                echo=False,\n",
        "                stop=[\"</s>\", \"[INST]\"]\n",
        "            )\n",
        "\n",
        "            retry_text = retry_response['choices'][0]['text'].strip()\n",
        "\n",
        "            if len(retry_text) >= 50 and \"[INST]\" not in retry_text:\n",
        "                return retry_text\n",
        "            else:\n",
        "                # Only use fallback as a last resort\n",
        "                print(\"Warning: Retry failed. Creating a dynamic fallback story.\")\n",
        "                # Create a more dynamic fallback story based on the prompt\n",
        "                return create_dynamic_fallback_story(prompt)\n",
        "\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in story generation: {e}\")\n",
        "        return create_dynamic_fallback_story(prompt)\n",
        "\n",
        "# Function to create a more varied fallback story when needed\n",
        "def create_dynamic_fallback_story(prompt):\n",
        "    # Extract potential characters or themes from the prompt\n",
        "    words = prompt.lower().split()\n",
        "\n",
        "    # Look for keywords that might be character types or settings\n",
        "    character_keywords = [\"about\", \"with\", \"featuring\", \"starring\", \"where\"]\n",
        "    setting_keywords = [\"in\", \"at\", \"during\", \"through\"]\n",
        "\n",
        "    characters = \"characters\"\n",
        "    setting = \"a mysterious place\"\n",
        "\n",
        "    # Extract potential character information\n",
        "    for keyword in character_keywords:\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            if idx + 1 < len(words):\n",
        "                characters = \" \".join(words[idx+1:min(idx+6, len(words))])\n",
        "                break\n",
        "\n",
        "    # Extract potential setting information\n",
        "    for keyword in setting_keywords:\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            if idx + 1 < len(words):\n",
        "                setting = \" \".join(words[idx+1:min(idx+4, len(words))])\n",
        "                break\n",
        "\n",
        "    # Create one of several story templates using the extracted information\n",
        "    import random\n",
        "    templates = [\n",
        "        f\"The sun cast long shadows across {setting} as {characters} embarked on an unexpected journey. \"\n",
        "        f\"What began as an ordinary day quickly transformed when they discovered a hidden secret that would change everything. \"\n",
        "        f\"Through challenges and revelations, they found strength in each other they never knew existed. \"\n",
        "        f\"By the journey's end, nothing would ever be the same for them again.\",\n",
        "\n",
        "        f\"A chance encounter in {setting} brought {characters} together at precisely the right moment. \"\n",
        "        f\"Neither could have anticipated how their lives would intertwine, nor the adventure that awaited them. \"\n",
        "        f\"Facing impossible odds, they were forced to trust each other despite their differences. \"\n",
        "        f\"Their story became one that would be told for generations to come.\",\n",
        "\n",
        "        f\"Deep within {setting}, {characters} discovered something that should have remained hidden. \"\n",
        "        f\"As they unraveled ancient mysteries, tension grew between them, testing the limits of their relationship. \"\n",
        "        f\"A moment of truth revealed their true character when everything was at stake. \"\n",
        "        f\"In the end, they emerged transformed, carrying both scars and wisdom from their extraordinary experience.\"\n",
        "    ]\n",
        "\n",
        "    # Return a randomly selected template\n",
        "    return random.choice(templates)\n",
        "\n",
        "# Function to split the story into panels\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    # First check if we have enough content to split\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    # Try to split by paragraphs first for more natural divisions\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]  # Remove empty paragraphs\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        # We have enough paragraphs, so use them as natural divisions\n",
        "        if len(paragraphs) > num_panels:\n",
        "            # Combine paragraphs to match the desired number of panels\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "            return panels\n",
        "        else:\n",
        "            # Perfect match or fewer paragraphs than panels\n",
        "            return paragraphs[:num_panels]\n",
        "\n",
        "    # If we don't have enough paragraphs, fall back to sentence splitting\n",
        "    sentences = []\n",
        "    # Split by common sentence endings\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]  # Remove empty or too-short sentences\n",
        "\n",
        "    # Handle edge case where there are fewer sentences than panels\n",
        "    if len(sentences) < num_panels:\n",
        "        # Create more sentences by splitting the longest ones\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            # Split at a comma or other natural break point if possible\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                # If no good split point, duplicate the sentence with a variation\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    # Calculate how many sentences per panel for a balanced distribution\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        # For the last panel, include any remaining sentences\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "    return panels\n",
        "\n",
        "# Load the Stable Diffusion XL model\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available, otherwise use CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Function to generate comic panels from any story prompt\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\"):\n",
        "    # Make sure the story prompt is compelling\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        # Load the Llama model\n",
        "        print(\"Loading Llama model...\")\n",
        "        llama_model = load_llama_model(model_path)\n",
        "\n",
        "        # Generate story from prompt with multiple attempts if needed\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(llama_model, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # Check if story generated properly\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Trying one more time with different approach.\")\n",
        "            # Try a more direct approach with the model\n",
        "            direct_prompt = f\"\"\"<s>[INST] Write a creative short story for children about {story_prompt}. Make it exciting with interesting characters and events. [/INST]\"\"\"\n",
        "\n",
        "            retry_response = llama_model(\n",
        "                direct_prompt,\n",
        "                max_tokens=600,\n",
        "                temperature=0.8,\n",
        "                top_p=0.9,\n",
        "                echo=False,\n",
        "                stop=[\"</s>\", \"[INST]\"]\n",
        "            )\n",
        "\n",
        "            story_text = retry_response['choices'][0]['text'].strip()\n",
        "\n",
        "            if len(story_text) < 50:\n",
        "                print(\"Still unable to generate a good story. Using the last fallback option.\")\n",
        "                story_text = (\n",
        "                    f\"In a world where anything was possible, {story_prompt} became reality. \"\n",
        "                    f\"The adventure began unexpectedly one morning when strange events unfolded. \"\n",
        "                    f\"Characters found themselves facing mysterious challenges that required courage and wit. \"\n",
        "                    f\"After many twists and turns, they discovered a solution that brought everything to a satisfying conclusion.\"\n",
        "                )\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating story: {e}\")\n",
        "        # Provide a fallback story\n",
        "        story_text = (\n",
        "            f\"In a world where anything was possible, {story_prompt} became reality. \"\n",
        "            f\"The adventure began unexpectedly one morning when strange events unfolded. \"\n",
        "            f\"Characters found themselves facing mysterious challenges that required courage and wit. \"\n",
        "            f\"After many twists and turns, they discovered a solution that brought everything to a satisfying conclusion.\"\n",
        "        )\n",
        "\n",
        "    # Split story into panels\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "\n",
        "    try:\n",
        "        # Load SDXL model\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "        for i, panel_text in enumerate(story_panels):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Create a prompt that helps SDXL understand it's a comic panel\n",
        "            # Use the art style specified by the user\n",
        "            prompt = f\"A detailed {art_style} illustration for a comic panel showing: {panel_text} cinematic lighting, expressive characters, vibrant colors\"\n",
        "\n",
        "            # Generate the image\n",
        "            image = pipe(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "            panel_images.append(image)\n",
        "\n",
        "            # Save the image\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Comic generation complete! All panels saved as PNG files.\")\n",
        "        return story_text, story_panels, panel_images\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a unique story and convert it into comic panels\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Write a story about a brave cat who learns to fly\",\n",
        "                    lines=2\n",
        "                )\n",
        "                num_panels = gr.Slider(minimum=2, maximum=8, value=4, step=1, label=\"Number of Panels\")\n",
        "                model_path = gr.Textbox(label=\"Optional: Custom Llama Model Path\", placeholder=\"Leave empty for default\")\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\n",
        "                        \"cartoon style\",\n",
        "                        \"manga style\",\n",
        "                        \"comic book style\",\n",
        "                        \"watercolor style\",\n",
        "                        \"3D rendered\",\n",
        "                        \"pixel art style\",\n",
        "                        \"oil painting style\",\n",
        "                        \"storybook illustration style\"\n",
        "                    ],\n",
        "                    value=\"cartoon style\",\n",
        "                    label=\"Art Style\"\n",
        "                )\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                story_output = gr.Textbox(label=\"Generated Story\", lines=8)\n",
        "\n",
        "        # Fixed Gallery component\n",
        "        panel_gallery = gr.Gallery(label=\"Comic Panels\", columns=2, rows=2, height=600)\n",
        "\n",
        "        # Status output for error messages\n",
        "        status_output = gr.Textbox(label=\"Status\", lines=2)\n",
        "\n",
        "        def run_generation(prompt, panels, path, style):\n",
        "            if not prompt:\n",
        "                return \"Please enter a story prompt\", [], \"Error: No prompt provided\"\n",
        "\n",
        "            model_path_to_use = path if path.strip() else None\n",
        "\n",
        "            try:\n",
        "                story_text, story_panels, images = generate_comic(prompt, int(panels), model_path_to_use, style)\n",
        "\n",
        "                if story_text and images:\n",
        "                    status = \"Success: Comic generated successfully!\"\n",
        "                elif story_text:\n",
        "                    status = \"Warning: Story generated but image creation failed\"\n",
        "                else:\n",
        "                    status = \"Error: Story generation failed\"\n",
        "\n",
        "                return story_text, images if images else [], status\n",
        "            except Exception as e:\n",
        "                return f\"An error occurred: {str(e)}\", [], f\"Error: {str(e)}\"\n",
        "\n",
        "        generate_button.click(\n",
        "            run_generation,\n",
        "            inputs=[story_prompt, num_panels, model_path, art_style],\n",
        "            outputs=[story_output, panel_gallery, status_output]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Main function to either run the generator directly or launch the web UI\n",
        "def main(use_web_ui=True):\n",
        "    if use_web_ui:\n",
        "        app = create_ui()\n",
        "        app.launch(share=True)  # Enable sharing for remote access\n",
        "    else:\n",
        "        # Example without web UI\n",
        "        story_prompt = input(\"Enter a story prompt: \")\n",
        "        num_panels = int(input(\"Enter number of panels (2-8): \"))\n",
        "        art_style = input(\"Enter art style (e.g., cartoon style, manga style): \")\n",
        "\n",
        "        story, panels, images = generate_comic(story_prompt, num_panels, None, art_style)\n",
        "\n",
        "        if images:\n",
        "            for i, img in enumerate(images):\n",
        "                plt.figure(figsize=(8, 8))\n",
        "                plt.imshow(img)\n",
        "                plt.title(f\"Panel {i+1}\")\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(use_web_ui=True)  # Set to False to use command line instead of web UI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "GupLMv2bPyIb",
        "outputId": "fbcb810f-149a-4a28-bdc8-ec2c1b5c56f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://99acdb205225c2de02.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://99acdb205225c2de02.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    # If no model path provided, use default path\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    \"\"\"Extract potential character names from the prompt\"\"\"\n",
        "    # Common superhero/character names to check for\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    # Check if any known character names are in the prompt\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    # Otherwise extract words that might be names (capitalized words)\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    \"\"\"Extract the main topic from the prompt\"\"\"\n",
        "    # Remove common instruction words\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "\n",
        "    # Look for interesting keywords\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    # Return the cleaned prompt if no specific keyword found\n",
        "    return cleaned.strip()\n",
        "\n",
        "def create_structured_childrens_story(character=\"the character\", topic=\"having an adventure\"):\n",
        "    \"\"\"Create a structured children's story with simple language and short sentences\"\"\"\n",
        "\n",
        "    # Clean up inputs\n",
        "    if not character or len(character) < 2:\n",
        "        character = \"the little hero\"\n",
        "\n",
        "    # Create a story with the structure of the Peter Parker example\n",
        "    story = f\"\"\"The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "# Enhanced story generation function using fine-tuned model\n",
        "def generate_story(model, tokenizer, prompt, max_length=400):\n",
        "    # Format prompt in a way that's more suitable for children's stories\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    try:\n",
        "        # Encode the prompt\n",
        "        input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "\n",
        "        # Move to the same device as the model\n",
        "        device = next(model.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        # Generate text with different parameters more suited to simple children's stories\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            top_k=40,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,  # Lower temperature for more consistent, simpler text\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Check quality of response\n",
        "        if len(generated_text) < 50 or \"In a world where anything was possible\" in generated_text:\n",
        "            print(\"Warning: Low quality story generated. Generating a structured children's story...\")\n",
        "\n",
        "            # Create a more structured children's story instead of using the default template\n",
        "            character_name = extract_character_name(prompt) or \"the character\"\n",
        "            topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "            return create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in story generation: {e}\")\n",
        "        return create_structured_childrens_story(extract_character_name(prompt), extract_topic(prompt))\n",
        "\n",
        "# Function to create a fallback story when needed\n",
        "def create_dynamic_fallback_story(prompt):\n",
        "    # Extract character and topics\n",
        "    character = extract_character_name(prompt) or \"the character\"\n",
        "    topic = extract_topic(prompt) or \"having fun\"\n",
        "\n",
        "    # Create a story similar to the Peter Parker example\n",
        "    story = f\"\"\"\n",
        "The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "# Function to split the story into panels (same as in your original code)\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    # First check if we have enough content to split\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    # Try to split by paragraphs first for more natural divisions\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]  # Remove empty paragraphs\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        # We have enough paragraphs, so use them as natural divisions\n",
        "        if len(paragraphs) > num_panels:\n",
        "            # Combine paragraphs to match the desired number of panels\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "            return panels\n",
        "        else:\n",
        "            # Perfect match or fewer paragraphs than panels\n",
        "            return paragraphs[:num_panels]\n",
        "\n",
        "    # If we don't have enough paragraphs, fall back to sentence splitting\n",
        "    sentences = []\n",
        "    # Split by common sentence endings\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]  # Remove empty or too-short sentences\n",
        "\n",
        "    # Handle edge case where there are fewer sentences than panels\n",
        "    if len(sentences) < num_panels:\n",
        "        # Create more sentences by splitting the longest ones\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            # Split at a comma or other natural break point if possible\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                # If no good split point, duplicate the sentence with a variation\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    # Calculate how many sentences per panel for a balanced distribution\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        # For the last panel, include any remaining sentences\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "    return panels\n",
        "\n",
        "# Load the Stable Diffusion XL model (same as in your original code)\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available, otherwise use CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Function to generate comic panels from story prompt using our fine-tuned model\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\"):\n",
        "    # Make sure the story prompt is compelling\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        # Load the fine-tuned model\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        # Generate story from prompt with multiple attempts if needed\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # Check if story generated properly\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Trying one more time with different approach.\")\n",
        "            # Try a more direct approach with the model\n",
        "            direct_prompt = f\"Write a simple children's story about {story_prompt}. Use short sentences.\"\n",
        "\n",
        "            story_text = generate_story(model, tokenizer, direct_prompt, max_length=500)\n",
        "\n",
        "            if len(story_text) < 50:\n",
        "                print(\"Still unable to generate a good story. Using the structured children's story.\")\n",
        "                character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "                topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "                story_text = create_structured_childrens_story(character_name, topic)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating story: {e}\")\n",
        "        # Provide a fallback story using our improved fallback generator\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "        topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "        story_text = create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "    # Split story into panels\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "\n",
        "    try:\n",
        "        # Load SDXL model\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "        for i, panel_text in enumerate(story_panels):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Create a prompt that helps SDXL understand it's a comic panel for children\n",
        "            # Use the art style specified by the user, with emphasis on children's book style\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text} bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            # Generate the image\n",
        "            image = pipe(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "            panel_images.append(image)\n",
        "\n",
        "            # Save the image\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Comic generation complete! All panels saved as PNG files.\")\n",
        "        return story_text, story_panels, panel_images\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story using your fine-tuned model and convert it into comic panels\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Write a story about a brave cat who learns to fly\",\n",
        "                    lines=2\n",
        "                )\n",
        "                num_panels = gr.Slider(minimum=2, maximum=8, value=4, step=1, label=\"Number of Panels\")\n",
        "                model_path = gr.Textbox(\n",
        "                    label=\"Custom Model Path\",\n",
        "                    placeholder=\"/content/drive/MyDrive/fine_tuned_tiny_stories_model\",\n",
        "                    value=\"/content/drive/MyDrive/fine_tuned_tiny_stories_model\"\n",
        "                )\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\n",
        "                        \"cartoon style\",\n",
        "                        \"manga style\",\n",
        "                        \"comic book style\",\n",
        "                        \"watercolor style\",\n",
        "                        \"3D rendered\",\n",
        "                        \"pixel art style\",\n",
        "                        \"oil painting style\",\n",
        "                        \"storybook illustration style\"\n",
        "                    ],\n",
        "                    value=\"cartoon style\",\n",
        "                    label=\"Art Style\"\n",
        "                )\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                story_output = gr.Textbox(label=\"Generated Story\", lines=8)\n",
        "\n",
        "        # Gallery component for displaying panels\n",
        "        panel_gallery = gr.Gallery(label=\"Comic Panels\", columns=2, rows=2, height=600)\n",
        "\n",
        "        # Status output for error messages\n",
        "        status_output = gr.Textbox(label=\"Status\", lines=2)\n",
        "\n",
        "        def run_generation(prompt, panels, path, style):\n",
        "            if not prompt:\n",
        "                return \"Please enter a story prompt\", [], \"Error: No prompt provided\"\n",
        "\n",
        "            model_path_to_use = path if path.strip() else None\n",
        "\n",
        "            try:\n",
        "                story_text, story_panels, images = generate_comic(prompt, int(panels), model_path_to_use, style)\n",
        "\n",
        "                if story_text and images:\n",
        "                    status = \"Success: Comic generated successfully!\"\n",
        "                elif story_text:\n",
        "                    status = \"Warning: Story generated but image creation failed\"\n",
        "                else:\n",
        "                    status = \"Error: Story generation failed\"\n",
        "\n",
        "                return story_text, images if images else [], status\n",
        "            except Exception as e:\n",
        "                return f\"An error occurred: {str(e)}\", [], f\"Error: {str(e)}\"\n",
        "\n",
        "        generate_button.click(\n",
        "            run_generation,\n",
        "            inputs=[story_prompt, num_panels, model_path, art_style],\n",
        "            outputs=[story_output, panel_gallery, status_output]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Main function to either run the generator directly or launch the web UI\n",
        "def main(use_web_ui=True):\n",
        "    if use_web_ui:\n",
        "        app = create_ui()\n",
        "        app.launch(share=True)  # Enable sharing for remote access\n",
        "    else:\n",
        "        # Example without web UI\n",
        "        story_prompt = input(\"Enter a story prompt: \")\n",
        "        num_panels = int(input(\"Enter number of panels (2-8): \"))\n",
        "        model_path = input(\"Enter model path (leave empty for default): \")\n",
        "        art_style = input(\"Enter art style (e.g., cartoon style, manga style): \")\n",
        "\n",
        "        story, panels, images = generate_comic(story_prompt, num_panels, model_path or None, art_style)\n",
        "\n",
        "        if images:\n",
        "            for i, img in enumerate(images):\n",
        "                plt.figure(figsize=(8, 8))\n",
        "                plt.imshow(img)\n",
        "                plt.title(f\"Panel {i+1}\")\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "if _name_ == \"_main_\":\n",
        "    main(use_web_ui=True)  # Set to False to use command line instead of web UI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "3bpN81sRRxQD",
        "outputId": "37010ff1-2c33-4271-dd76-299dc900c5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '_name_' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-118bfab1b385>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m_name_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"_main_\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_web_ui\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set to False to use command line instead of web UI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_name_' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    # If no model path provided, use default path\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    \"\"\"Extract potential character names from the prompt\"\"\"\n",
        "    # Common superhero/character names to check for\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    # Check if any known character names are in the prompt\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    # Otherwise extract words that might be names (capitalized words)\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    \"\"\"Extract the main topic from the prompt\"\"\"\n",
        "    # Remove common instruction words\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "\n",
        "    # Look for interesting keywords\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    # Return the cleaned prompt if no specific keyword found\n",
        "    return cleaned.strip()\n",
        "\n",
        "def create_structured_childrens_story(character=\"the character\", topic=\"having an adventure\"):\n",
        "    \"\"\"Create a structured children's story with simple language and short sentences\"\"\"\n",
        "\n",
        "    # Clean up inputs\n",
        "    if not character or len(character) < 2:\n",
        "        character = \"the little hero\"\n",
        "\n",
        "    # Create a story with the structure of the Peter Parker example\n",
        "    story = f\"\"\"The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "# Enhanced story generation function using fine-tuned model\n",
        "def generate_story(model, tokenizer, prompt, max_length=400):\n",
        "    # Format prompt in a way that's more suitable for children's stories\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    try:\n",
        "        # Encode the prompt\n",
        "        input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "\n",
        "        # Move to the same device as the model\n",
        "        device = next(model.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        # Generate text with different parameters more suited to simple children's stories\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            top_k=40,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,  # Lower temperature for more consistent, simpler text\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Check quality of response\n",
        "        if len(generated_text) < 50 or \"In a world where anything was possible\" in generated_text:\n",
        "            print(\"Warning: Low quality story generated. Generating a structured children's story...\")\n",
        "\n",
        "            # Create a more structured children's story instead of using the default template\n",
        "            character_name = extract_character_name(prompt) or \"the character\"\n",
        "            topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "            return create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in story generation: {e}\")\n",
        "        return create_structured_childrens_story(extract_character_name(prompt), extract_topic(prompt))\n",
        "\n",
        "# Function to create a fallback story when needed\n",
        "def create_dynamic_fallback_story(prompt):\n",
        "    # Extract character and topics\n",
        "    character = extract_character_name(prompt) or \"the character\"\n",
        "    topic = extract_topic(prompt) or \"having fun\"\n",
        "\n",
        "    # Create a story similar to the Peter Parker example\n",
        "    story = f\"\"\"\n",
        "The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "# Function to split the story into panels and also extract dialogues for speech bubbles\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    # First check if we have enough content to split\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    # Try to split by paragraphs first for more natural divisions\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]  # Remove empty paragraphs\n",
        "\n",
        "    # Extract dialogues for speech bubbles\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        # We have enough paragraphs, so use them as natural divisions\n",
        "        if len(paragraphs) > num_panels:\n",
        "            # Combine paragraphs to match the desired number of panels\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                # Extract dialogue (text within quotes) for speech bubbles\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                # Remove dialogue from panel text for better image prompts\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            # Perfect match or fewer paragraphs than panels\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    # If we don't have enough paragraphs, fall back to sentence splitting\n",
        "    sentences = []\n",
        "    # Split by common sentence endings\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]  # Remove empty or too-short sentences\n",
        "\n",
        "    # Handle edge case where there are fewer sentences than panels\n",
        "    if len(sentences) < num_panels:\n",
        "        # Create more sentences by splitting the longest ones\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            # Split at a comma or other natural break point if possible\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                # If no good split point, duplicate the sentence with a variation\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    # Calculate how many sentences per panel for a balanced distribution\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        # For the last panel, include any remaining sentences\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        # Extract dialogue for speech bubbles\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        # Remove dialogue from panel text for better image prompts\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "# Function to extract dialogue from text (text within quotes)\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        # Return the first dialogue found\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "# Load the Stable Diffusion XL model\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available, otherwise use CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Function to add speech bubbles to images\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    # Convert PIL image to numpy array for OpenCV operations\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    # If position not provided, place the bubble at a suitable position\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    # Create a transparent layer for the bubble\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    # Calculate bubble size based on text length\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    # Draw speech bubble\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    # Add a pointer to make it look like a speech bubble\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    # Draw text\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    # Try to load a font, fall back to default if not available\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    # Wrap text to fit in the bubble\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    # Calculate text position for centering\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    # Draw the wrapped text\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    # Combine the original image with the bubble\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "# Function to wrap text to fit within a certain width\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        # Test if adding this word would exceed the width\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            line_width = font.getsize(test_line)[0]\n",
        "        except:\n",
        "            # If getsize not available, estimate based on character count\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    # Add the last line\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "# Function to generate comic panels from story prompt using our fine-tuned model\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\"):\n",
        "    # Make sure the story prompt is compelling\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        # Load the fine-tuned model\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        # Generate story from prompt with multiple attempts if needed\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # Check if story generated properly\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Trying one more time with different approach.\")\n",
        "            # Try a more direct approach with the model\n",
        "            direct_prompt = f\"Write a simple children's story about {story_prompt}. Use short sentences.\"\n",
        "\n",
        "            story_text = generate_story(model, tokenizer, direct_prompt, max_length=500)\n",
        "\n",
        "            if len(story_text) < 50:\n",
        "                print(\"Still unable to generate a good story. Using the structured children's story.\")\n",
        "                character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "                topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "                story_text = create_structured_childrens_story(character_name, topic)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating story: {e}\")\n",
        "        # Provide a fallback story using our improved fallback generator\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "        topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "        story_text = create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "    # Split story into panels and extract dialogues\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        # Load SDXL model\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        # Extract character name for consistency\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        # Generate a base character image for consistency across panels\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "        base_image = pipe(\n",
        "            prompt=base_prompt,\n",
        "            negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "            num_inference_steps=30\n",
        "        ).images[0]\n",
        "\n",
        "        # Resize base image for consistency\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        # Generate each panel\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Create a prompt that helps SDXL understand it's a comic panel for children\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            # Generate the image using the base character for consistency\n",
        "            image = pipe(\n",
        "                prompt=prompt,\n",
        "                image=base_image,  # Use base image for character consistency\n",
        "                strength=0.7,      # Keep some elements of base image but allow for new scene\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "\n",
        "            # Resize to standard panel size\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            # Add speech bubble if there's dialogue\n",
        "            if dialogues[i]:\n",
        "                # Calculate a position for the speech bubble based on panel number\n",
        "                position = (256, 100 + (i % 2) * 50)  # Vary position slightly between panels\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            # Save the individual panel\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        # Create a combined comic strip\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))  # Calculate how many rows we need\n",
        "        columns = min(2, num_panels)         # Max 2 columns\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        # Save the combined comic strip\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Write a story about a brave cat who learns to fly\",\n",
        "                    lines=2\n",
        "                )\n",
        "                num_panels = gr.Slider(minimum=2, maximum=8, value=4, step=1, label=\"Number of Panels\")\n",
        "                model_path = gr.Textbox(\n",
        "                    label=\"Custom Model Path\",\n",
        "                    placeholder=\"/content/drive/MyDrive/fine_tuned_tiny_stories_model\",\n",
        "                    value=\"/content/drive/MyDrive/fine_tuned_tiny_stories_model\"\n",
        "                )\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\n",
        "                        \"cartoon style\",\n",
        "                        \"manga style\",\n",
        "                        \"comic book style\",\n",
        "                        \"watercolor style\",\n",
        "                        \"3D rendered\",\n",
        "                        \"pixel art style\",\n",
        "                        \"oil painting style\",\n",
        "                        \"storybook illustration style\"\n",
        "                    ],\n",
        "                    value=\"cartoon style\",\n",
        "                    label=\"Art Style\"\n",
        "                )\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                story_output = gr.Textbox(label=\"Generated Story\", lines=8)\n",
        "\n",
        "        # Gallery component for displaying individual panels\n",
        "        panel_gallery = gr.Gallery(label=\"Comic Panels\", columns=2, rows=2, height=500)\n",
        "\n",
        "        # Image component for displaying the combined comic strip\n",
        "        comic_strip_image = gr.Image(label=\"Combined Comic Strip\", height=600)\n",
        "\n",
        "        # Status output for error messages\n",
        "        status_output = gr.Textbox(label=\"Status\", lines=2)\n",
        "\n",
        "        def run_generation(prompt, panels, path, style):\n",
        "            if not prompt:\n",
        "                return \"Please enter a story prompt\", [], None, \"Error: No prompt provided\"\n",
        "\n",
        "            model_path_to_use = path if path.strip() else None\n",
        "\n",
        "            try:\n",
        "                story_text, story_panels, images, comic_strip = generate_comic(prompt, int(panels), model_path_to_use, style)\n",
        "\n",
        "                if story_text and images and comic_strip:\n",
        "                    status = \"Success: Comic generated successfully with speech bubbles!\"\n",
        "                elif story_text and images:\n",
        "                    status = \"Success: Individual panels generated but combined strip failed\"\n",
        "                elif story_text:\n",
        "                    status = \"Warning: Story generated but image creation failed\"\n",
        "                else:\n",
        "                    status = \"Error: Story generation failed\"\n",
        "\n",
        "                return story_text, images if images else [], comic_strip, status\n",
        "            except Exception as e:\n",
        "                return f\"An error occurred: {str(e)}\", [], None, f\"Error: {str(e)}\"\n",
        "\n",
        "        generate_button.click(\n",
        "            run_generation,\n",
        "            inputs=[story_prompt, num_panels, model_path, art_style],\n",
        "            outputs=[story_output, panel_gallery, comic_strip_image, status_output]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Main function to either run the generator directly or launch the web UI\n",
        "def main(use_web_ui=True):\n",
        "    if use_web_ui:\n",
        "        app = create_ui()\n",
        "        app.launch(share=True)  # Enable sharing for remote access\n",
        "    else:\n",
        "        # Example without web UI\n",
        "        story_prompt = input(\"Enter a story prompt: \")\n",
        "        num_panels = int(input(\"Enter number of panels (2-8): \"))\n",
        "        model_path = input(\"Enter model path (leave empty for default): \")\n",
        "        art_style = input(\"Enter art style (e.g., cartoon style, manga style): \")\n",
        "\n",
        "        story, panels, images, comic_strip = generate_comic(story_prompt, num_panels, model_path or None, art_style)\n",
        "\n",
        "        if images:\n",
        "            for i, img in enumerate(images):\n",
        "                plt.figure(figsize=(8, 8))\n",
        "                plt.imshow(img)\n",
        "                plt.title(f\"Panel {i+1}\")\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "            # Display combined comic strip\n",
        "            if comic_strip:\n",
        "                plt.figure(figsize=(12, 12))\n",
        "                plt.imshow(comic_strip)\n",
        "                plt.title(\"Complete Comic Strip\")\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(use_web_ui=True)  # Set to False to use command line instead of web UI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "AmAk1MrRTbzD",
        "outputId": "e21b81e4-b201-4740-c333-66bcc1547be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f55e72022b729581bb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f55e72022b729581bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    # If no model path provided, use default path\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    \"\"\"Extract potential character names from the prompt\"\"\"\n",
        "    # Common superhero/character names to check for\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    # Check if any known character names are in the prompt\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    # Otherwise extract words that might be names (capitalized words)\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    \"\"\"Extract the main topic from the prompt\"\"\"\n",
        "    # Remove common instruction words\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "\n",
        "    # Look for interesting keywords\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    # Return the cleaned prompt if no specific keyword found\n",
        "    return cleaned.strip()\n",
        "\n",
        "def create_structured_childrens_story(character=\"the character\", topic=\"having an adventure\"):\n",
        "    \"\"\"Create a structured children's story with simple language and short sentences\"\"\"\n",
        "\n",
        "    # Clean up inputs\n",
        "    if not character or len(character) < 2:\n",
        "        character = \"the little hero\"\n",
        "\n",
        "    # Create a story with the structure of the Peter Parker example\n",
        "    story = f\"\"\"The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "# Enhanced story generation function using fine-tuned model\n",
        "def generate_story(model, tokenizer, prompt, max_length=400):\n",
        "    # Format prompt in a way that's more suitable for children's stories\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    try:\n",
        "        # Encode the prompt\n",
        "        input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "\n",
        "        # Move to the same device as the model\n",
        "        device = next(model.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        # Generate text with different parameters more suited to simple children's stories\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            top_k=40,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,  # Lower temperature for more consistent, simpler text\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Check quality of response\n",
        "        if len(generated_text) < 50 or \"In a world where anything was possible\" in generated_text:\n",
        "            print(\"Warning: Low quality story generated. Generating a structured children's story...\")\n",
        "\n",
        "            # Create a more structured children's story instead of using the default template\n",
        "            character_name = extract_character_name(prompt) or \"the character\"\n",
        "            topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "            return create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in story generation: {e}\")\n",
        "        return create_structured_childrens_story(extract_character_name(prompt), extract_topic(prompt))\n",
        "\n",
        "# Function to create a fallback story when needed\n",
        "def create_dynamic_fallback_story(prompt):\n",
        "    # Extract character and topics\n",
        "    character = extract_character_name(prompt) or \"the character\"\n",
        "    topic = extract_topic(prompt) or \"having fun\"\n",
        "\n",
        "    # Create a story similar to the Peter Parker example\n",
        "    story = f\"\"\"\n",
        "The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "# Function to split the story into panels and also extract dialogues for speech bubbles\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    # First check if we have enough content to split\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    # Try to split by paragraphs first for more natural divisions\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]  # Remove empty paragraphs\n",
        "\n",
        "    # Extract dialogues for speech bubbles\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        # We have enough paragraphs, so use them as natural divisions\n",
        "        if len(paragraphs) > num_panels:\n",
        "            # Combine paragraphs to match the desired number of panels\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                # Extract dialogue (text within quotes) for speech bubbles\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                # Remove dialogue from panel text for better image prompts\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            # Perfect match or fewer paragraphs than panels\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    # If we don't have enough paragraphs, fall back to sentence splitting\n",
        "    sentences = []\n",
        "    # Split by common sentence endings\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]  # Remove empty or too-short sentences\n",
        "\n",
        "    # Handle edge case where there are fewer sentences than panels\n",
        "    if len(sentences) < num_panels:\n",
        "        # Create more sentences by splitting the longest ones\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            # Split at a comma or other natural break point if possible\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                # If no good split point, duplicate the sentence with a variation\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    # Calculate how many sentences per panel for a balanced distribution\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        # For the last panel, include any remaining sentences\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        # Extract dialogue for speech bubbles\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        # Remove dialogue from panel text for better image prompts\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "# Function to extract dialogue from text (text within quotes)\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        # Return the first dialogue found\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "# Load the custom InkoloRA model\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    # Define custom model path\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    # Check if the model file exists\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        # Load the model - adjust the model_id based on what base model your safetensors file is trained on\n",
        "        # Assuming it's SD 1.5 based; modify if it's based on a different architecture\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        # Load the custom weights\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        # Check if CUDA is available, otherwise use CPU\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "# Original SDXL model load function as fallback\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available, otherwise use CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Function to add speech bubbles to images\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    # Convert PIL image to numpy array for OpenCV operations\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    # If position not provided, place the bubble at a suitable position\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    # Create a transparent layer for the bubble\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    # Calculate bubble size based on text length\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    # Draw speech bubble\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    # Add a pointer to make it look like a speech bubble\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    # Draw text\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    # Try to load a font, fall back to default if not available\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    # Wrap text to fit in the bubble\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    # Calculate text position for centering\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    # Draw the wrapped text\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    # Combine the original image with the bubble\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "# Function to wrap text to fit within a certain width\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        # Test if adding this word would exceed the width\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            line_width = font.getsize(test_line)[0]\n",
        "        except:\n",
        "            # If getsize not available, estimate based on character count\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    # Add the last line\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "# Function to generate comic panels from story prompt using our fine-tuned model\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\"):\n",
        "    # Make sure the story prompt is compelling\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        # Load the fine-tuned model\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        # Generate story from prompt with multiple attempts if needed\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # Check if story generated properly\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Trying one more time with different approach.\")\n",
        "            # Try a more direct approach with the model\n",
        "            direct_prompt = f\"Write a simple children's story about {story_prompt}. Use short sentences.\"\n",
        "\n",
        "            story_text = generate_story(model, tokenizer, direct_prompt, max_length=500)\n",
        "\n",
        "            if len(story_text) < 50:\n",
        "                print(\"Still unable to generate a good story. Using the structured children's story.\")\n",
        "                character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "                topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "                story_text = create_structured_childrens_story(character_name, topic)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating story: {e}\")\n",
        "        # Provide a fallback story using our improved fallback generator\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "        topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "        story_text = create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "    # Split story into panels and extract dialogues\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        # Load our custom InkoloRA model instead of SDXL\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        # Extract character name for consistency\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        # Generate a base character image for consistency across panels\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        # Check if using StableDiffusionXLPipeline or StableDiffusionPipeline (custom model may use different API)\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            # For regular SD pipeline\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        # Resize base image for consistency\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        # Generate each panel\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Create a prompt that helps the model understand it's a comic panel for children\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            # Generate the image using the base character for consistency\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,  # Use base image for character consistency\n",
        "                    strength=0.7,      # Keep some elements of base image but allow for new scene\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                # For regular SD pipeline\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,  # Use base image for character consistency\n",
        "                    strength=0.7,      # Keep some elements of base image but allow for new scene\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            # Resize to standard panel size\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            # Add speech bubble if there's dialogue\n",
        "            if dialogues[i]:\n",
        "                # Calculate a position for the speech bubble based on panel number\n",
        "                position = (256, 100 + (i % 2) * 50)  # Vary position slightly between panels\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            # Save the individual panel\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        # Create a combined comic strip\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))  # Calculate how many rows we need\n",
        "        columns = min(2, num_panels)         # Max 2 columns\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        # Save the combined comic strip\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Write a story about a brave cat who learns to fly\",\n",
        "                    lines=2\n",
        "                )\n",
        "                num_panels = gr.Slider(minimum=2, maximum=8, value=4, step=1, label=\"Number of Panels\")\n",
        "                model_path = gr.Textbox(\n",
        "                    label=\"Story Model Path (optional)\",\n",
        "                    placeholder=\"/content/drive/MyDrive/fine_tuned_tiny_stories_model\",\n",
        "                    value=\"/content/drive/MyDrive/fine_tuned_tiny_stories_model\"\n",
        "                )\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\n",
        "                        \"cartoon style\",\n",
        "                        \"manga style\",\n",
        "                        \"comic book style\",\n",
        "                        \"watercolor style\",\n",
        "                        \"3D rendered\",\n",
        "                        \"pixel art style\",\n",
        "                        \"oil painting style\",\n",
        "                        \"storybook illustration style\",\n",
        "                        \"Inkolo style\" # Added new option for your custom model's style\n",
        "                    ],\n",
        "                    value=\"Inkolo style\", # Set your custom style as default\n",
        "                    label=\"Art Style\"\n",
        "                )\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                story_output = gr.Textbox(label=\"Generated Story\", lines=8)\n",
        "\n",
        "        # Gallery component for displaying individual panels\n",
        "        panel_gallery = gr.Gallery(label=\"Comic Panels\", columns=2, rows=2, height=500)\n",
        "\n",
        "        # Image component for displaying the combined comic strip\n",
        "        comic_strip_image = gr.Image(label=\"Combined Comic Strip\", height=600)\n",
        "\n",
        "        # Status output for error messages\n",
        "        status_output = gr.Textbox(label=\"Status\", lines=2)\n",
        "\n",
        "        def run_generation(prompt, panels, path, style):\n",
        "            if not prompt:\n",
        "                return \"Please enter a story prompt\", [], None, \"Error: No prompt provided\"\n",
        "\n",
        "            model_path_to_use = path if path.strip() else None\n",
        "\n",
        "            try:\n",
        "                story_text, story_panels, images, comic_strip = generate_comic(prompt, int(panels), model_path_to_use, style)\n",
        "\n",
        "                if story_text and images and comic_strip:\n",
        "                    status = \"Success: Comic generated successfully with InkoloRA model and speech bubbles!\"\n",
        "                elif story_text and images:\n",
        "                    status = \"Success: Individual panels generated but combined strip failed\"\n",
        "                elif story_text:\n",
        "                    status = \"Warning: Story generated but image creation failed\"\n",
        "                else:\n",
        "                    status = \"Error: Story generation failed\"\n",
        "\n",
        "                return story_text, images if images else [], comic_strip, status\n",
        "            except Exception as e:\n",
        "                return f\"An error occurred: {str(e)}\", [], None, f\"Error: {str(e)}\"\n",
        "\n",
        "        generate_button.click(\n",
        "            run_generation,\n",
        "            inputs=[story_prompt, num_panels, model_path, art_style],\n",
        "            outputs=[story_output, panel_gallery, comic_strip_image, status_output]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Main function to either run the generator directly or launch the web UI\n",
        "def main(use_web_ui=True):\n",
        "    # Mount Google Drive to access the models\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        print(\"Continuing without mounting drive - this may cause issues accessing models\")\n",
        "\n",
        "    if use_web_ui:\n",
        "        app = create_ui()\n",
        "        app.launch(share=True)  # Enable sharing for remote access\n",
        "    else:\n",
        "        # Example without web UI\n",
        "        story_prompt = input(\"Enter a story prompt: \")\n",
        "        num_panels = int(input(\"Enter number of panels (2-8): \"))\n",
        "        model_path = input(\"Enter story model path (leave empty for default): \")\n",
        "        art_style = input(\"Enter art style (e.g., cartoon style, manga style, Inkolo style): \")\n",
        "\n",
        "        story, panels, images, comic_strip = generate_comic(story_prompt, num_panels, model_path or None, art_style)\n",
        "\n",
        "        if images:\n",
        "            for i, img in enumerate(images):\n",
        "                plt.figure(figsize=(8, 8))\n",
        "                plt.imshow(img)\n",
        "                plt.title(f\"Panel {i+1}\")\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "            # Display combined comic strip\n",
        "            if comic_strip:\n",
        "                plt.figure(figsize=(12, 12))\n",
        "                plt.imshow(comic_strip)\n",
        "                plt.title(\"Complete Comic Strip\")\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(use_web_ui=True)  # Set to False to use command line instead of web UI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "OUrVlYuWVqqo",
        "outputId": "2142b4cf-2c32-4aa0-b4c7-6abb44399c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gradio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-97f2e642320b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStableDiffusionPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStableDiffusionXLPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpmwFJWmoSnJ",
        "outputId": "09a95d2c-b47d-4055-d928-42e9abc68722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.23.3-py3-none-any.whl (46.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    # If no model path provided, use default path\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    \"\"\"Extract potential character names from the prompt\"\"\"\n",
        "    # Common superhero/character names to check for\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    # Check if any known character names are in the prompt\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    # Otherwise extract words that might be names (capitalized words)\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    \"\"\"Extract the main topic from the prompt\"\"\"\n",
        "    # Remove common instruction words\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "\n",
        "    # Look for interesting keywords\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    # Return the cleaned prompt if no specific keyword found\n",
        "    return cleaned.strip()\n",
        "\n",
        "def create_structured_childrens_story(character=\"the character\", topic=\"having an adventure\"):\n",
        "    \"\"\"Create a structured children's story with simple language and short sentences\"\"\"\n",
        "\n",
        "    # Clean up inputs\n",
        "    if not character or len(character) < 2:\n",
        "        character = \"the little hero\"\n",
        "\n",
        "    # Create a story with the structure of the Peter Parker example\n",
        "    story = f\"\"\"The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "# Enhanced story generation function using fine-tuned model\n",
        "def generate_story(model, tokenizer, prompt, max_length=400):\n",
        "    # Format prompt in a way that's more suitable for children's stories\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    try:\n",
        "        # Encode the prompt\n",
        "        input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "\n",
        "        # Move to the same device as the model\n",
        "        device = next(model.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        # Generate text with different parameters more suited to simple children's stories\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            top_k=40,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,  # Lower temperature for more consistent, simpler text\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Check quality of response\n",
        "        if len(generated_text) < 50 or \"In a world where anything was possible\" in generated_text:\n",
        "            print(\"Warning: Low quality story generated. Generating a structured children's story...\")\n",
        "\n",
        "            # Create a more structured children's story instead of using the default template\n",
        "            character_name = extract_character_name(prompt) or \"the character\"\n",
        "            topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "            return create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in story generation: {e}\")\n",
        "        return create_structured_childrens_story(extract_character_name(prompt), extract_topic(prompt))\n",
        "\n",
        "# Function to create a fallback story when needed\n",
        "def create_dynamic_fallback_story(prompt):\n",
        "    # Extract character and topics\n",
        "    character = extract_character_name(prompt) or \"the character\"\n",
        "    topic = extract_topic(prompt) or \"having fun\"\n",
        "\n",
        "    # Create a story similar to the Peter Parker example\n",
        "    story = f\"\"\"\n",
        "The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "# Function to split the story into panels and also extract dialogues for speech bubbles\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    # First check if we have enough content to split\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    # Try to split by paragraphs first for more natural divisions\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]  # Remove empty paragraphs\n",
        "\n",
        "    # Extract dialogues for speech bubbles\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        # We have enough paragraphs, so use them as natural divisions\n",
        "        if len(paragraphs) > num_panels:\n",
        "            # Combine paragraphs to match the desired number of panels\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                # Extract dialogue (text within quotes) for speech bubbles\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                # Remove dialogue from panel text for better image prompts\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            # Perfect match or fewer paragraphs than panels\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    # If we don't have enough paragraphs, fall back to sentence splitting\n",
        "    sentences = []\n",
        "    # Split by common sentence endings\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]  # Remove empty or too-short sentences\n",
        "\n",
        "    # Handle edge case where there are fewer sentences than panels\n",
        "    if len(sentences) < num_panels:\n",
        "        # Create more sentences by splitting the longest ones\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            # Split at a comma or other natural break point if possible\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                # If no good split point, duplicate the sentence with a variation\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    # Calculate how many sentences per panel for a balanced distribution\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        # For the last panel, include any remaining sentences\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        # Extract dialogue for speech bubbles\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        # Remove dialogue from panel text for better image prompts\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "# Function to extract dialogue from text (text within quotes)\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        # Return the first dialogue found\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "# Load the custom InkoloRA model\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    # Define custom model path\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    # Check if the model file exists\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        # Load the model - adjust the model_id based on what base model your safetensors file is trained on\n",
        "        # Assuming it's SD 1.5 based; modify if it's based on a different architecture\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        # Load the custom weights\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        # Check if CUDA is available, otherwise use CPU\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "# Original SDXL model load function as fallback\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available, otherwise use CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Function to add speech bubbles to images\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    # Convert PIL image to numpy array for OpenCV operations\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    # If position not provided, place the bubble at a suitable position\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    # Create a transparent layer for the bubble\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    # Calculate bubble size based on text length\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    # Draw speech bubble\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    # Add a pointer to make it look like a speech bubble\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    # Draw text\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    # Try to load a font, fall back to default if not available\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    # Wrap text to fit in the bubble\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    # Calculate text position for centering\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    # Draw the wrapped text\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    # Combine the original image with the bubble\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "# Function to wrap text to fit within a certain width\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        # Test if adding this word would exceed the width\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            line_width = font.getsize(test_line)[0]\n",
        "        except:\n",
        "            # If getsize not available, estimate based on character count\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    # Add the last line\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "# Function to generate comic panels from story prompt using our fine-tuned model\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\"):\n",
        "    # Make sure the story prompt is compelling\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        # Load the fine-tuned model\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        # Generate story from prompt with multiple attempts if needed\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # Check if story generated properly\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Trying one more time with different approach.\")\n",
        "            # Try a more direct approach with the model\n",
        "            direct_prompt = f\"Write a simple children's story about {story_prompt}. Use short sentences.\"\n",
        "\n",
        "            story_text = generate_story(model, tokenizer, direct_prompt, max_length=500)\n",
        "\n",
        "            if len(story_text) < 50:\n",
        "                print(\"Still unable to generate a good story. Using the structured children's story.\")\n",
        "                character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "                topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "                story_text = create_structured_childrens_story(character_name, topic)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating story: {e}\")\n",
        "        # Provide a fallback story using our improved fallback generator\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "        topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "        story_text = create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "    # Split story into panels and extract dialogues\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        # Load our custom InkoloRA model instead of SDXL\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        # Extract character name for consistency\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        # Generate a base character image for consistency across panels\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        # Check if using StableDiffusionXLPipeline or StableDiffusionPipeline (custom model may use different API)\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            # For regular SD pipeline\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        # Resize base image for consistency\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        # Generate each panel\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Create a prompt that helps the model understand it's a comic panel for children\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            # Generate the image using the base character for consistency\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,  # Use base image for character consistency\n",
        "                    strength=0.7,      # Keep some elements of base image but allow for new scene\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                # For regular SD pipeline\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,  # Use base image for character consistency\n",
        "                    strength=0.7,      # Keep some elements of base image but allow for new scene\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            # Resize to standard panel size\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            # Add speech bubble if there's dialogue\n",
        "            if dialogues[i]:\n",
        "                # Calculate a position for the speech bubble based on panel number\n",
        "                position = (256, 100 + (i % 2) * 50)  # Vary position slightly between panels\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            # Save the individual panel\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        # Create a combined comic strip\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))  # Calculate how many rows we need\n",
        "        columns = min(2, num_panels)         # Max 2 columns\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        # Save the combined comic strip\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Write a story about a brave cat who learns to fly\",\n",
        "                    lines=2\n",
        "                )\n",
        "                num_panels = gr.Slider(minimum=2, maximum=8, value=4, step=1, label=\"Number of Panels\")\n",
        "                model_path = gr.Textbox(\n",
        "                    label=\"Story Model Path (optional)\",\n",
        "                    placeholder=\"/content/drive/MyDrive/fine_tuned_tiny_stories_model\",\n",
        "                    value=\"/content/drive/MyDrive/fine_tuned_tiny_stories_model\"\n",
        "                )\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\n",
        "                        \"cartoon style\",\n",
        "                        \"manga style\",\n",
        "                        \"comic book style\",\n",
        "                        \"watercolor style\",\n",
        "                        \"3D rendered\",\n",
        "                        \"pixel art style\",\n",
        "                        \"oil painting style\",\n",
        "                        \"storybook illustration style\",\n",
        "                        \"Inkolo style\" # Added new option for your custom model's style\n",
        "                    ],\n",
        "                    value=\"Inkolo style\", # Set your custom style as default\n",
        "                    label=\"Art Style\"\n",
        "                )\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                story_output = gr.Textbox(label=\"Generated Story\", lines=8)\n",
        "\n",
        "        # Gallery component for displaying individual panels\n",
        "        panel_gallery = gr.Gallery(label=\"Comic Panels\", columns=2, rows=2, height=500)\n",
        "\n",
        "        # Image component for displaying the combined comic strip\n",
        "        comic_strip_image = gr.Image(label=\"Combined Comic Strip\", height=600)\n",
        "\n",
        "        # Status output for error messages\n",
        "        status_output = gr.Textbox(label=\"Status\", lines=2)\n",
        "\n",
        "        def run_generation(prompt, panels, path, style):\n",
        "            if not prompt:\n",
        "                return \"Please enter a story prompt\", [], None, \"Error: No prompt provided\"\n",
        "\n",
        "            model_path_to_use = path if path.strip() else None\n",
        "\n",
        "            try:\n",
        "                story_text, story_panels, images, comic_strip = generate_comic(prompt, int(panels), model_path_to_use, style)\n",
        "\n",
        "                if story_text and images and comic_strip:\n",
        "                    status = \"Success: Comic generated successfully with InkoloRA model and speech bubbles!\"\n",
        "                elif story_text and images:\n",
        "                    status = \"Success: Individual panels generated but combined strip failed\"\n",
        "                elif story_text:\n",
        "                    status = \"Warning: Story generated but image creation failed\"\n",
        "                else:\n",
        "                    status = \"Error: Story generation failed\"\n",
        "\n",
        "                return story_text, images if images else [], comic_strip, status\n",
        "            except Exception as e:\n",
        "                return f\"An error occurred: {str(e)}\", [], None, f\"Error: {str(e)}\"\n",
        "\n",
        "        generate_button.click(\n",
        "            run_generation,\n",
        "            inputs=[story_prompt, num_panels, model_path, art_style],\n",
        "            outputs=[story_output, panel_gallery, comic_strip_image, status_output]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Main function to either run the generator directly or launch the web UI\n",
        "def main(use_web_ui=True):\n",
        "    # Mount Google Drive to access the models\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        print(\"Continuing without mounting drive - this may cause issues accessing models\")\n",
        "\n",
        "    if use_web_ui:\n",
        "        app = create_ui()\n",
        "        app.launch(share=True)  # Enable sharing for remote access\n",
        "    else:\n",
        "        # Example without web UI\n",
        "        story_prompt = input(\"Enter a story prompt: \")\n",
        "        num_panels = int(input(\"Enter number of panels (2-8): \"))\n",
        "        model_path = input(\"Enter story model path (leave empty for default): \")\n",
        "        art_style = input(\"Enter art style (e.g., cartoon style, manga style, Inkolo style): \")\n",
        "\n",
        "        story, panels, images, comic_strip = generate_comic(story_prompt, num_panels, model_path or None, art_style)\n",
        "\n",
        "        if images:\n",
        "            for i, img in enumerate(images):\n",
        "                plt.figure(figsize=(8, 8))\n",
        "                plt.imshow(img)\n",
        "                plt.title(f\"Panel {i+1}\")\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "            # Display combined comic strip\n",
        "            if comic_strip:\n",
        "                plt.figure(figsize=(12, 12))\n",
        "                plt.imshow(comic_strip)\n",
        "                plt.title(\"Complete Comic Strip\")\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(use_web_ui=True)  # Set to False to use command line instead of web UI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "273rTyt3osee",
        "outputId": "de126c6b-9d3b-4c89-fab3-ee9c1dba658a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://dbb76da67e00670c30.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dbb76da67e00670c30.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    # If no model path provided, use default path\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    \"\"\"Extract potential character names from the prompt\"\"\"\n",
        "    # Common superhero/character names to check for\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    # Check if any known character names are in the prompt\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    # Otherwise extract words that might be names (capitalized words)\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    \"\"\"Extract the main topic from the prompt\"\"\"\n",
        "    # Remove common instruction words\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "\n",
        "    # Look for interesting keywords\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    # Return the cleaned prompt if no specific keyword found\n",
        "    return cleaned.strip()\n",
        "\n",
        "def create_structured_childrens_story(character=\"the character\", topic=\"having an adventure\"):\n",
        "    \"\"\"Create a structured children's story with simple language and short sentences\"\"\"\n",
        "\n",
        "    # Clean up inputs\n",
        "    if not character or len(character) < 2:\n",
        "        character = \"the little hero\"\n",
        "\n",
        "    # Create a story with the structure of the Peter Parker example\n",
        "    story = f\"\"\"The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "# Enhanced story generation function using fine-tuned model\n",
        "def generate_story(model, tokenizer, prompt, max_length=400):\n",
        "    # Format prompt in a way that's more suitable for children's stories\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    try:\n",
        "        # Encode the prompt\n",
        "        input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "\n",
        "        # Move to the same device as the model\n",
        "        device = next(model.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        # Generate text with different parameters more suited to simple children's stories\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            top_k=40,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,  # Lower temperature for more consistent, simpler text\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Check quality of response\n",
        "        if len(generated_text) < 50 or \"In a world where anything was possible\" in generated_text:\n",
        "            print(\"Warning: Low quality story generated. Generating a structured children's story...\")\n",
        "\n",
        "            # Create a more structured children's story instead of using the default template\n",
        "            character_name = extract_character_name(prompt) or \"the character\"\n",
        "            topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "            return create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in story generation: {e}\")\n",
        "        return create_structured_childrens_story(extract_character_name(prompt), extract_topic(prompt))\n",
        "\n",
        "# Function to split the story into panels and also extract dialogues for speech bubbles\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    # First check if we have enough content to split\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    # Try to split by paragraphs first for more natural divisions\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]  # Remove empty paragraphs\n",
        "\n",
        "    # Extract dialogues for speech bubbles\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        # We have enough paragraphs, so use them as natural divisions\n",
        "        if len(paragraphs) > num_panels:\n",
        "            # Combine paragraphs to match the desired number of panels\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                # Extract dialogue (text within quotes) for speech bubbles\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                # Remove dialogue from panel text for better image prompts\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            # Perfect match or fewer paragraphs than panels\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    # If we don't have enough paragraphs, fall back to sentence splitting\n",
        "    sentences = []\n",
        "    # Split by common sentence endings\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]  # Remove empty or too-short sentences\n",
        "\n",
        "    # Handle edge case where there are fewer sentences than panels\n",
        "    if len(sentences) < num_panels:\n",
        "        # Create more sentences by splitting the longest ones\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            # Split at a comma or other natural break point if possible\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                # If no good split point, duplicate the sentence with a variation\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    # Calculate how many sentences per panel for a balanced distribution\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        # For the last panel, include any remaining sentences\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        # Extract dialogue for speech bubbles\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        # Remove dialogue from panel text for better image prompts\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "# Function to extract dialogue from text (text within quotes)\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        # Return the first dialogue found\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "# Load the custom InkoloRA model\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    # Define custom model path\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    # Check if the model file exists\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        # Load the model - adjust the model_id based on what base model your safetensors file is trained on\n",
        "        # Assuming it's SD 1.5 based; modify if it's based on a different architecture\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        # Load the custom weights\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        # Check if CUDA is available, otherwise use CPU\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "# Original SDXL model load function as fallback\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available, otherwise use CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Function to add speech bubbles to images\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    # Convert PIL image to numpy array for OpenCV operations\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    # If position not provided, place the bubble at a suitable position\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    # Create a transparent layer for the bubble\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    # Calculate bubble size based on text length\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    # Draw speech bubble\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    # Add a pointer to make it look like a speech bubble\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    # Draw text\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    # Try to load a font, fall back to default if not available\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    # Wrap text to fit in the bubble\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    # Calculate text position for centering\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    # Draw the wrapped text\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    # Combine the original image with the bubble\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "# Function to wrap text to fit within a certain width\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        # Test if adding this word would exceed the width\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            # Use getbbox or getlength instead of deprecated getsize\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]  # right - left\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]  # Fallback to deprecated method\n",
        "        except:\n",
        "            # If all methods fail, estimate based on character count\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    # Add the last line\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "# NEW FEATURE: Predefined comic templates\n",
        "def get_template_data(template_name):\n",
        "    \"\"\"Return predefined prompts and dialogue for various comic templates\"\"\"\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)]\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)]\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "# Function to generate comic using a template\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        # Load SDXL model (templates work better with SDXL)\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        # Generate a base character image for consistency\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        # Check if using SDXL or regular SD pipeline\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        # Resize base image for consistency\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        # Get the prompts, dialogues, and positions from template data\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "\n",
        "        # Fill with defaults if not enough data\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        # Generate each panel\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            # Resize to standard panel size\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            # Add speech bubble if there's dialogue\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            # Save the individual panel\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        # Create a combined comic strip\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        # Save the combined comic strip\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        # Create a dummy story text for consistency with the generate_comic function\n",
        "        story_text = f\"A {template_name} adventure comic generated using a template.\"\n",
        "        story_panels = prompts\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "# Function to get available templates\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "# Function to generate comic panels from story prompt using our fine-tuned model\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    # Check if using a template\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    # Original story-based comic generation\n",
        "    # Make sure the story prompt is compelling\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        # Load the fine-tuned model\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        # Generate story from prompt with multiple attempts if needed\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # Check if story generated properly\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Trying one more time with different approach.\")\n",
        "            # Try a more direct approach with the model\n",
        "            direct_prompt = f\"Write a simple children's story about {story_prompt}. Use short sentences.\"\n",
        "\n",
        "            story_text = generate_story(model, tokenizer, direct_prompt, max_length=500)\n",
        "\n",
        "            if len(story_text) < 50:\n",
        "                print(\"Still unable to generate a good story. Using the structured children's story.\")\n",
        "                character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "                topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "                story_text = create_structured_childrens_story(character_name, topic)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating story: {e}\")\n",
        "        # Provide a fallback story using our improved fallback generator\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "        topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "        story_text = create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "    # Split story into panels and extract dialogues\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        # Load our custom InkoloRA model instead of SDXL\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        # Extract character name for consistency\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        # Generate a base character image for consistency across panels\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        # Check if using StableDiffusionXLPipeline or StableDiffusionPipeline (custom model may use different API)\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            # For regular SD pipeline\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        # Resize base image for consistency\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        # Generate each panel\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Create a prompt that helps the model understand it's a comic panel for children\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            # Generate the image using the base character for consistency\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,  # Use base image for character consistency\n",
        "                    strength=0.7,      # Keep some elements of base image but allow for new scene\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                # For regular SD pipeline\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,  # Use base image for character consistency\n",
        "                    strength=0.7,      # Keep some elements of base image but allow for new scene\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            # Resize to standard panel size\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            # Add speech bubble if there's dialogue\n",
        "            if dialogues[i]:\n",
        "                # Calculate a position for the speech bubble based on panel number\n",
        "                position = (256, 100 + (i % 2) * 50)  # Vary position slightly between panels\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            # Save the individual panel\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        # Create a combined comic strip\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))  # Calculate how many rows we need\n",
        "        columns = min(2, num_panels)         # Max 2 columns\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        # Save the combined comic strip\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # Add template selection dropdown\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "r7JewXl-o3OY",
        "outputId": "f8664394-a336-4173-cdf1-793aaf6ee57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-4-298a8b64de67>, line 720)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-298a8b64de67>\"\u001b[0;36m, line \u001b[0;32m720\u001b[0m\n\u001b[0;31m    choices=[\"None\"] + get_available_templates(),\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    # If no model path provided, use default path\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    \"\"\"Extract potential character names from the prompt\"\"\"\n",
        "    # Common superhero/character names to check for\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    # Check if any known character names are in the prompt\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    # Otherwise extract words that might be names (capitalized words)\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    \"\"\"Extract the main topic from the prompt\"\"\"\n",
        "    # Remove common instruction words\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "\n",
        "    # Look for interesting keywords\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    # Return the cleaned prompt if no specific keyword found\n",
        "    return cleaned.strip()\n",
        "\n",
        "def create_structured_childrens_story(character=\"the character\", topic=\"having an adventure\"):\n",
        "    \"\"\"Create a structured children's story with simple language and short sentences\"\"\"\n",
        "\n",
        "    # Clean up inputs\n",
        "    if not character or len(character) < 2:\n",
        "        character = \"the little hero\"\n",
        "\n",
        "    # Create a story with the structure of the Peter Parker example\n",
        "    story = f\"\"\"The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "# Enhanced story generation function using fine-tuned model\n",
        "def generate_story(model, tokenizer, prompt, max_length=400):\n",
        "    # Format prompt in a way that's more suitable for children's stories\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    try:\n",
        "        # Encode the prompt\n",
        "        input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "\n",
        "        # Move to the same device as the model\n",
        "        device = next(model.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        # Generate text with different parameters more suited to simple children's stories\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            top_k=40,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,  # Lower temperature for more consistent, simpler text\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Check quality of response\n",
        "        if len(generated_text) < 50 or \"In a world where anything was possible\" in generated_text:\n",
        "            print(\"Warning: Low quality story generated. Generating a structured children's story...\")\n",
        "\n",
        "            # Create a more structured children's story instead of using the default template\n",
        "            character_name = extract_character_name(prompt) or \"the character\"\n",
        "            topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "            return create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in story generation: {e}\")\n",
        "        return create_structured_childrens_story(extract_character_name(prompt), extract_topic(prompt))\n",
        "\n",
        "# Function to split the story into panels and also extract dialogues for speech bubbles\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    # First check if we have enough content to split\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    # Try to split by paragraphs first for more natural divisions\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]  # Remove empty paragraphs\n",
        "\n",
        "    # Extract dialogues for speech bubbles\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        # We have enough paragraphs, so use them as natural divisions\n",
        "        if len(paragraphs) > num_panels:\n",
        "            # Combine paragraphs to match the desired number of panels\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                # Extract dialogue (text within quotes) for speech bubbles\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                # Remove dialogue from panel text for better image prompts\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            # Perfect match or fewer paragraphs than panels\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    # If we don't have enough paragraphs, fall back to sentence splitting\n",
        "    sentences = []\n",
        "    # Split by common sentence endings\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]  # Remove empty or too-short sentences\n",
        "\n",
        "    # Handle edge case where there are fewer sentences than panels\n",
        "    if len(sentences) < num_panels:\n",
        "        # Create more sentences by splitting the longest ones\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            # Split at a comma or other natural break point if possible\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                # If no good split point, duplicate the sentence with a variation\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    # Calculate how many sentences per panel for a balanced distribution\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        # For the last panel, include any remaining sentences\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        # Extract dialogue for speech bubbles\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        # Remove dialogue from panel text for better image prompts\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "# Function to extract dialogue from text (text within quotes)\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        # Return the first dialogue found\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "# Load the custom InkoloRA model\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    # Define custom model path\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    # Check if the model file exists\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        # Load the model - adjust the model_id based on what base model your safetensors file is trained on\n",
        "        # Assuming it's SD 1.5 based; modify if it's based on a different architecture\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        # Load the custom weights\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        # Check if CUDA is available, otherwise use CPU\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "# Original SDXL model load function as fallback\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available, otherwise use CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Function to add speech bubbles to images\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    # Convert PIL image to numpy array for OpenCV operations\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    # If position not provided, place the bubble at a suitable position\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    # Create a transparent layer for the bubble\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    # Calculate bubble size based on text length\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    # Draw speech bubble\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    # Add a pointer to make it look like a speech bubble\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    # Draw text\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    # Try to load a font, fall back to default if not available\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    # Wrap text to fit in the bubble\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    # Calculate text position for centering\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    # Draw the wrapped text\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    # Combine the original image with the bubble\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "# Function to wrap text to fit within a certain width\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        # Test if adding this word would exceed the width\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            # Use getbbox or getlength instead of deprecated getsize\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]  # right - left\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]  # Fallback to deprecated method\n",
        "        except:\n",
        "            # If all methods fail, estimate based on character count\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    # Add the last line\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "# NEW FEATURE: Predefined comic templates\n",
        "def get_template_data(template_name):\n",
        "    \"\"\"Return predefined prompts and dialogue for various comic templates\"\"\"\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)]\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)]\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "# Function to generate comic using a template\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        # Load SDXL model (templates work better with SDXL)\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        # Generate a base character image for consistency\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        # Check if using SDXL or regular SD pipeline\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        # Resize base image for consistency\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        # Get the prompts, dialogues, and positions from template data\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "\n",
        "        # Fill with defaults if not enough data\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        # Generate each panel\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            # Resize to standard panel size\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            # Add speech bubble if there's dialogue\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            # Save the individual panel\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        # Create a combined comic strip\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        # Save the combined comic strip\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        # Create a dummy story text for consistency with the generate_comic function\n",
        "        story_text = f\"A {template_name} adventure comic generated using a template.\"\n",
        "        story_panels = prompts\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "# Function to get available templates\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "# Function to generate comic panels from story prompt using our fine-tuned model\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    # Check if using a template\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    # Original story-based comic generation\n",
        "    # Make sure the story prompt is compelling\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        # Load the fine-tuned model\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        # Generate story from prompt with multiple attempts if needed\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # Check if story generated properly\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Trying one more time with different approach.\")\n",
        "            # Try a more direct approach with the model\n",
        "            direct_prompt = f\"Write a simple children's story about {story_prompt}. Use short sentences.\"\n",
        "\n",
        "            story_text = generate_story(model, tokenizer, direct_prompt, max_length=500)\n",
        "\n",
        "            if len(story_text) < 50:\n",
        "                print(\"Still unable to generate a good story. Using the structured children's story.\")\n",
        "                character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "                topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "                story_text = create_structured_childrens_story(character_name, topic)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating story: {e}\")\n",
        "        # Provide a fallback story using our improved fallback generator\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "        topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "        story_text = create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "    # Split story into panels and extract dialogues\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        # Load our custom InkoloRA model instead of SDXL\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        # Extract character name for consistency\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        # Generate a base character image for consistency across panels\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        # Check if using StableDiffusionXLPipeline or StableDiffusionPipeline (custom model may use different API)\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            # For regular SD pipeline\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        # Resize base image for consistency\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        # Generate each panel\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Create a prompt that helps the model understand it's a comic panel for children\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            # Generate the image using the base character for consistency\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,  # Use base image for character consistency\n",
        "                    strength=0.7,      # Keep some elements of base image but allow for new scene\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                # For regular SD pipeline\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,  # Use base image for character consistency\n",
        "                    strength=0.7,      # Keep some elements of base image but allow for new scene\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            # Resize to standard panel size\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            # Add speech bubble if there's dialogue\n",
        "            if dialogues[i]:\n",
        "                # Calculate a position for the speech bubble based on panel number\n",
        "                position = (256, 100 + (i % 2) * 50)  # Vary position slightly between panels\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            # Save the individual panel\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        # Create a combined comic strip\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))  # Calculate how many rows we need\n",
        "        columns = min(2, num_panels)         # Max 2 columns\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        # Save the combined comic strip\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # Add template selection dropdown\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),\n",
        "                    label=\"Choose Template (optional)\",\n",
        "                    value=\"None\"\n",
        "                )\n",
        "\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to fly')\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                num_panels = gr.Slider(\n",
        "                    label=\"Number of Panels\",\n",
        "                    minimum=2,\n",
        "                    maximum=8,\n",
        "                    value=4,\n",
        "                    step=1\n",
        "                )\n",
        "\n",
        "                art_style = gr.Dropdown(\n",
        "                                        choices=[\"Cartoon Style\", \"Realistic\", \"Anime\", \"Sketch\"],\n",
        "                    label=\"Art Style\",\n",
        "                    value=\"Cartoon Style\"\n",
        "                )\n",
        "\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"Generated Story\",\n",
        "                    placeholder=\"The generated story will appear here...\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                output_panels = gr.Gallery(\n",
        "                    label=\"Comic Panels\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"comic-panels\",\n",
        "                    height=200\n",
        "                ).style(grid=2)\n",
        "\n",
        "                output_comic = gr.Image(\n",
        "                    label=\"Combined Comic Strip\",\n",
        "                    type=\"pil\"\n",
        "                )\n",
        "\n",
        "        # Define the action for the button click\n",
        "        def generate_comic_action(story_prompt, num_panels, art_style, use_template):\n",
        "            story_text, story_panels, panel_images, comic_strip = generate_comic(\n",
        "                story_prompt, num_panels, art_style=art_style, use_template=use_template\n",
        "            )\n",
        "            output_text.update(story_text)\n",
        "            output_panels.update(panel_images)\n",
        "            output_comic.update(comic_strip)\n",
        "            return\n",
        "\n",
        "        generate_button.click(\n",
        "            generate_comic_action,\n",
        "            inputs=[story_prompt, num_panels, art_style, template_dropdown],\n",
        "            outputs=[output_text, output_panels, output_comic]\n",
        "        )\n",
        "\n",
        "    app.launch()\n",
        "\n",
        "# Call the function to create the UI\n",
        "create_ui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "MNsAhhnJufrH",
        "outputId": "c4414962-b3a8-4aea-cab6-a622720a8216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Gallery' object has no attribute 'style'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0e7d779761e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;31m# Call the function to create the UI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m \u001b[0mcreate_ui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-0e7d779761e4>\u001b[0m in \u001b[0;36mcreate_ui\u001b[0;34m()\u001b[0m\n\u001b[1;32m    758\u001b[0m                     \u001b[0melem_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"comic-panels\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m                     \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m                 ).style(grid=2)\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 output_comic = gr.Image(\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Gallery' object has no attribute 'style'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def create_structured_childrens_story(character=\"the character\", topic=\"having an adventure\"):\n",
        "    if not character or len(character) < 2:\n",
        "        character = \"the little hero\"\n",
        "\n",
        "    story = f\"\"\"The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "def generate_story(model, tokenizer, prompt, max_length=400):\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    try:\n",
        "        input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "        device = next(model.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            top_k=40,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        if len(generated_text) < 50 or \"In a world where anything was possible\" in generated_text:\n",
        "            print(\"Warning: Low quality story generated. Generating a structured children's story...\")\n",
        "            character_name = extract_character_name(prompt) or \"the character\"\n",
        "            topic = extract_topic(prompt) or \"having an adventure\"\n",
        "            return create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in story generation: {e}\")\n",
        "        return create_structured_childrens_story(extract_character_name(prompt), extract_topic(prompt))\n",
        "\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        if len(paragraphs) > num_panels:\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    sentences = []\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]\n",
        "\n",
        "    if len(sentences) < num_panels:\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]\n",
        "        except:\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "def get_template_data(template_name):\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)]\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)]\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        story_text = f\"A {template_name} adventure comic generated using a template.\"\n",
        "        story_panels = prompts\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Trying one more time with different approach.\")\n",
        "            direct_prompt = f\"Write a simple children's story about {story_prompt}. Use short sentences.\"\n",
        "\n",
        "            story_text = generate_story(model, tokenizer, direct_prompt, max_length=500)\n",
        "\n",
        "            if len(story_text) < 50:\n",
        "                print(\"Still unable to generate a good story. Using the structured children's story.\")\n",
        "                character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "                topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "                story_text = create_structured_childrens_story(character_name, topic)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating story: {e}\")\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "        topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "        story_text = create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                position = (256, 100 + (i % 2) * 50)\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),\n",
        "                    label=\"Choose Template (optional)\",\n",
        "                    value=\"None\"\n",
        "                )\n",
        "\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to fly')\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                num_panels = gr.Slider(\n",
        "                    label=\"Number of Panels\",\n",
        "                    minimum=2,\n",
        "                    maximum=8,\n",
        "                    value=4,\n",
        "                    step=1\n",
        "                )\n",
        "\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\"Cartoon Style\", \"Realistic\", \"Anime\", \"Sketch\"],\n",
        "                    label=\"Art Style\",\n",
        "                    value=\"Cartoon Style\"\n",
        "                )\n",
        "\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"Generated Story\",\n",
        "                    placeholder=\"The generated story will appear here...\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                output_panels = gr.Gallery(\n",
        "                    label=\"Comic Panels\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"comic-panels\",\n",
        "                    height=200,\n",
        "                    columns=2  # Set the number of columns for the gallery\n",
        "                )\n",
        "\n",
        "                output_comic = gr.Image(\n",
        "                    label=\"Combined Comic Strip\",\n",
        "                    type=\"pil\"\n",
        "                )\n",
        "\n",
        "        def generate_comic_action(story_prompt, num_panels, art_style, use_template):\n",
        "            story_text, story_panels, panel_images, comic_strip = generate_comic(\n",
        "                story_prompt, num_panels, art_style=art_style, use_template=use_template\n",
        "            )\n",
        "            output_text.update(story_text)\n",
        "            output_panels.update(panel_images)\n",
        "            output_comic.update(comic_strip)\n",
        "            return\n",
        "\n",
        "        generate_button.click(\n",
        "            generate_comic_action,\n",
        "            inputs=[story_prompt, num_panels, art_style, template_dropdown],\n",
        "            outputs=[output_text, output_panels, output_comic]\n",
        "        )\n",
        "\n",
        "    app.launch()\n",
        "\n",
        "# Call the function to create the UI\n",
        "create_ui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "p7zg-Glmwzci",
        "outputId": "54d783bc-45a7-4c13-f904-66fa73309751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0dfef397bacc03490e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0dfef397bacc03490e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def create_structured_childrens_story(character=\"the character\", topic=\"having an adventure\"):\n",
        "    if not character or len(character) < 2:\n",
        "        character = \"the little hero\"\n",
        "\n",
        "    story = f\"\"\"The story of {character} began in the park. {character} was a little child who loved to play with friends.\n",
        "\n",
        "One sunny day, a friend came over and said, \"{character}, you must come and play with me! I'm the best at {topic}!\"\n",
        "\n",
        "{character} was so excited. {character} jumped up and down with happiness. They all played together and had lots of fun.\n",
        "\n",
        "{character} learned how to be good at {topic}. Soon, everyone wanted to play with {character}.\n",
        "\n",
        "{character} and the friends became the best of friends. They played every day after school.\n",
        "\n",
        "{character} was so happy to have such good friends. They would be friends forever and have many more adventures together.\n",
        "\"\"\"\n",
        "    return story\n",
        "\n",
        "def generate_story(model, tokenizer, prompt, max_length=400):\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    try:\n",
        "        input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "        device = next(model.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            top_k=40,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        if len(generated_text) < 50 or \"In a world where anything was possible\" in generated_text:\n",
        "            print(\"Warning: Low quality story generated. Generating a structured children's story...\")\n",
        "            character_name = extract_character_name(prompt) or \"the character\"\n",
        "            topic = extract_topic(prompt) or \"having an adventure\"\n",
        "            return create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in story generation: {e}\")\n",
        "        return create_structured_childrens_story(extract_character_name(prompt), extract_topic(prompt))\n",
        "\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        if len(paragraphs) > num_panels:\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    sentences = []\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]\n",
        "\n",
        "    if len(sentences) < num_panels:\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]\n",
        "        except:\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "def get_template_data(template_name):\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)]\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)]\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        story_text = f\"A {template_name} adventure comic generated using a template.\"\n",
        "        story_panels = prompts\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Trying one more time with different approach.\")\n",
        "            direct_prompt = f\"Write a simple children's story about {story_prompt}. Use short sentences.\"\n",
        "\n",
        "            story_text = generate_story(model, tokenizer, direct_prompt, max_length=500)\n",
        "\n",
        "            if len(story_text) < 50:\n",
        "                print(\"Still unable to generate a good story. Using the structured children's story.\")\n",
        "                character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "                topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "                story_text = create_structured_childrens_story(character_name, topic)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating story: {e}\")\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "        topic = extract_topic(story_prompt) or \"having an adventure\"\n",
        "        story_text = create_structured_childrens_story(character_name, topic)\n",
        "\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                position = (256, 100 + (i % 2) * 50)\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),\n",
        "                    label=\"Choose Template (optional)\",\n",
        "                    value=\"None\"\n",
        "                )\n",
        "\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to fly')\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                num_panels = gr.Slider(\n",
        "                    label=\"Number of Panels\",\n",
        "                    minimum=2,\n",
        "                    maximum=8,\n",
        "                    value=4,\n",
        "                    step=1\n",
        "                )\n",
        "\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\"Cartoon Style\", \"Realistic\", \"Anime\", \"Sketch\"],\n",
        "                    label=\"Art Style\",\n",
        "                    value=\"Cartoon Style\"\n",
        "                )\n",
        "\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"Generated Story\",\n",
        "                    placeholder=\"The generated story will appear here...\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                output_panels = gr.Gallery(\n",
        "                    label=\"Comic Panels\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"comic-panels\",\n",
        "                    height=200,\n",
        "                    columns=2  # Set the number of columns for the gallery\n",
        "                )\n",
        "\n",
        "                output_comic = gr.Image(\n",
        "                    label=\"Combined Comic Strip\",\n",
        "                    type=\"pil\"\n",
        "                )\n",
        "\n",
        "        # Fixed function to handle the generation and properly return outputs\n",
        "        def generate_comic_action(story_prompt, num_panels, art_style, use_template):\n",
        "            story_text, story_panels, panel_images, comic_strip = generate_comic(\n",
        "                story_prompt, num_panels, art_style=art_style, use_template=use_template\n",
        "            )\n",
        "            # Return the outputs directly rather than updating them\n",
        "            return story_text, panel_images if panel_images else [], comic_strip\n",
        "\n",
        "        generate_button.click(\n",
        "            generate_comic_action,\n",
        "            inputs=[story_prompt, num_panels, art_style, template_dropdown],\n",
        "            outputs=[output_text, output_panels, output_comic]\n",
        "        )\n",
        "\n",
        "    app.launch()\n",
        "\n",
        "# Mount Google Drive if in Colab environment\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Not running in Colab or drive already mounted\")\n",
        "\n",
        "# Call the function to create the UI\n",
        "if __name__ == \"__main__\":\n",
        "    create_ui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "JHet_2xkxaCR",
        "outputId": "25f93941-b22f-45bd-8927-b011b071f583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://44957111b564a93361.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://44957111b564a93361.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Try loading a fallback model if the fine-tuned one fails\n",
        "        try:\n",
        "            print(\"Attempting to load fallback model gpt2...\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = model.to(device)\n",
        "            print(f\"Fallback model loaded successfully and moved to {device}\")\n",
        "            return model, tokenizer\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading fallback model: {e2}\")\n",
        "            return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_story(model, tokenizer, prompt, max_length=400, retries=3):\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    # Increase repetition penalty and adjust parameters for better generation\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "            device = next(model.parameters()).device\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Use different generation parameters for each retry attempt\n",
        "            temperature = 0.7 + (attempt * 0.1)  # Gradually increase temperature with each retry\n",
        "            repetition_penalty = 1.2 + (attempt * 0.1)  # Gradually increase repetition penalty\n",
        "\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=3,  # Increased from 2\n",
        "                top_k=50,  # Increased from 40\n",
        "                top_p=0.92,  # Adjusted from 0.9\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=repetition_penalty\n",
        "            )\n",
        "\n",
        "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Clean up the generated text to focus on the actual story\n",
        "            if story_prompt in generated_text:\n",
        "                generated_text = generated_text.replace(story_prompt, \"\").strip()\n",
        "\n",
        "            # Check if we have a reasonable story\n",
        "            if len(generated_text) > 100 and \".\" in generated_text:\n",
        "                print(f\"Successfully generated story on attempt {attempt+1}\")\n",
        "                return generated_text\n",
        "            else:\n",
        "                print(f\"Attempt {attempt+1} produced insufficient content ({len(generated_text)} chars). Retrying...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in story generation attempt {attempt+1}: {e}\")\n",
        "\n",
        "    # If all retries fail, generate a more varied story using a different approach\n",
        "    print(\"All generation attempts failed. Using alternative generation method...\")\n",
        "    return generate_alternative_story(prompt)\n",
        "\n",
        "def generate_alternative_story(prompt):\n",
        "    \"\"\"\n",
        "    Generate a more varied story when the model-based generation fails.\n",
        "    This creates a more dynamic story than the previous structured approach.\n",
        "    \"\"\"\n",
        "    character = extract_character_name(prompt) or \"the little hero\"\n",
        "    topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "    # Create a list of possible story templates\n",
        "    story_templates = [\n",
        "        f\"\"\"Once upon a time, there was {character} who lived in a small village.\n",
        "{character} loved {topic} more than anything else.\n",
        "\n",
        "One day, {character} discovered something magical while playing outside. It was a tiny glowing ball that could float in the air.\n",
        "\n",
        "\"What's this?\" wondered {character}. The ball started to move, and {character} followed it into the forest.\n",
        "\n",
        "The ball led {character} to a hidden garden where other children were playing with similar magical balls. They welcomed {character} and taught them new games.\n",
        "\n",
        "{character} had so much fun that day! When it was time to go home, one of the children gave {character} a magical ball of their own.\n",
        "\n",
        "From that day on, {character} visited the hidden garden often and made many new friends. They all loved {topic} together and had wonderful adventures.\"\"\",\n",
        "\n",
        "        f\"\"\"{character} woke up early one morning with a big smile. Today was the day for {topic}!\n",
        "\n",
        "{character} packed a small bag with snacks and a water bottle. \"I'm ready for my adventure!\" {character} said.\n",
        "\n",
        "Outside, {character} met a friendly cat. \"Hello, cat! Would you like to join me for {topic}?\" The cat meowed and followed along.\n",
        "\n",
        "As they walked through the park, they found a lost teddy bear. \"We should find who this belongs to,\" said {character}.\n",
        "\n",
        "They asked everyone they met about the teddy bear. Finally, they met a little girl who was crying. \"My teddy!\" she said happily when she saw it.\n",
        "\n",
        "\"Thank you for helping me find my teddy,\" the girl smiled. \"Would you like to play with me?\"\n",
        "\n",
        "{character}, the cat, and the girl spent the whole day playing and having fun with {topic}. It was the best day ever!\"\"\",\n",
        "\n",
        "        f\"\"\"The sun was shining brightly when {character} decided it was the perfect day for {topic}.\n",
        "\n",
        "\"I've never tried {topic} before,\" {character} thought, \"but today I will be brave!\"\n",
        "\n",
        "{character} began slowly, taking small steps. It wasn't easy at first, and {character} almost gave up.\n",
        "\n",
        "\"Don't worry, you can do it!\" said a friendly voice. {character} looked up and saw a smiling butterfly.\n",
        "\n",
        "With the butterfly's encouragement, {character} tried again. This time, {character} did much better!\n",
        "\n",
        "Soon, {character} was having so much fun with {topic}. \"Thank you for believing in me,\" {character} said to the butterfly.\n",
        "\n",
        "The butterfly fluttered around happily. \"Sometimes we just need a friend to help us be brave,\" it replied.\n",
        "\n",
        "{character} went home that evening with a happy heart, excited to try {topic} again tomorrow.\"\"\"\n",
        "    ]\n",
        "\n",
        "    import random\n",
        "    return random.choice(story_templates)\n",
        "\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        if len(paragraphs) > num_panels:\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    sentences = []\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]\n",
        "\n",
        "    if len(sentences) < num_panels:\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]\n",
        "        except:\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "def get_template_data(template_name):\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)]\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)]\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        story_text = f\"A {template_name} adventure comic generated using a template.\"\n",
        "        story_panels = prompts\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # If story generation still produces insufficient content after multiple attempts\n",
        "        # and with different parameters, try generating with a completely different approach\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Using alternative generation approach.\")\n",
        "            story_text = generate_alternative_story(story_prompt)\n",
        "            print(\"Alternative story generated:\")\n",
        "            print(story_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in model-based story generation: {e}\")\n",
        "        print(\"Using alternative story generation approach...\")\n",
        "        story_text = generate_alternative_story(story_prompt)\n",
        "\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Make the image prompts unique for each panel based on the actual story content\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                position = (256, 100 + (i % 2) * 50)\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),\n",
        "                    label=\"Choose Template (optional)\",\n",
        "                    value=\"None\"\n",
        "                )\n",
        "\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to fly')\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                num_panels = gr.Slider(\n",
        "                    label=\"Number of Panels\",\n",
        "                    minimum=2,\n",
        "                    maximum=8,\n",
        "                    value=4,\n",
        "                    step=1\n",
        "                )\n",
        "\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\"Cartoon Style\", \"Realistic\", \"Anime\", \"Sketch\"],\n",
        "                    label=\"Art Style\",\n",
        "                    value=\"Cartoon Style\"\n",
        "                )\n",
        "\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"Generated Story\",\n",
        "                    placeholder=\"The generated story will appear here...\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                output_panels = gr.Gallery(\n",
        "                    label=\"Comic Panels\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"comic-panels\",\n",
        "                    height=200,\n",
        "                    columns=2  # Set the number of columns for the gallery\n",
        "                )\n",
        "\n",
        "                output_comic = gr.Image(\n",
        "                    label=\"Combined Comic Strip\",\n",
        "                    type=\"pil\"\n",
        "                )\n",
        "\n",
        "        # Fixed function to handle the generation and properly return outputs\n",
        "        def generate_comic_action(story_prompt, num_panels, art_style, use_template):\n",
        "            story_text, story_panels, panel_images, comic_strip = generate_comic(\n",
        "                story_prompt, num_panels, art_style=art_style, use_template=use_template\n",
        "            )\n",
        "            # Return the outputs directly rather than updating them\n",
        "            return story_text, panel_images if panel_images else [], comic_strip\n",
        "\n",
        "        generate_button.click(\n",
        "            generate_comic_action,\n",
        "            inputs=[story_prompt, num_panels, art_style, template_dropdown],\n",
        "            outputs=[output_text, output_panels, output_comic]\n",
        "        )\n",
        "\n",
        "    app.launch()\n",
        "\n",
        "# Mount Google Drive if in Colab environment\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Not running in Colab or drive already mounted\")\n",
        "\n",
        "# Call the function to create the UI\n",
        "if __name__ == \"__main__\":\n",
        "    create_ui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "ZnuP7FCszuu0",
        "outputId": "592465ab-4b37-4a83-84bd-afde13ebf079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://be8fe34d6cbdceef8d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://be8fe34d6cbdceef8d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Try loading a fallback model if the fine-tuned one fails\n",
        "        try:\n",
        "            print(\"Attempting to load fallback model gpt2...\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = model.to(device)\n",
        "            print(f\"Fallback model loaded successfully and moved to {device}\")\n",
        "            return model, tokenizer\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading fallback model: {e2}\")\n",
        "            return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_story(model, tokenizer, prompt, max_length=400, retries=3):\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    # Increase repetition penalty and adjust parameters for better generation\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "            device = next(model.parameters()).device\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Use different generation parameters for each retry attempt\n",
        "            temperature = 0.7 + (attempt * 0.1)  # Gradually increase temperature with each retry\n",
        "            repetition_penalty = 1.2 + (attempt * 0.1)  # Gradually increase repetition penalty\n",
        "\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=3,  # Increased from 2\n",
        "                top_k=50,  # Increased from 40\n",
        "                top_p=0.92,  # Adjusted from 0.9\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=repetition_penalty\n",
        "            )\n",
        "\n",
        "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Clean up the generated text to focus on the actual story\n",
        "            if story_prompt in generated_text:\n",
        "                generated_text = generated_text.replace(story_prompt, \"\").strip()\n",
        "\n",
        "            # Check if we have a reasonable story\n",
        "            if len(generated_text) > 100 and \".\" in generated_text:\n",
        "                print(f\"Successfully generated story on attempt {attempt+1}\")\n",
        "                return generated_text\n",
        "            else:\n",
        "                print(f\"Attempt {attempt+1} produced insufficient content ({len(generated_text)} chars). Retrying...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in story generation attempt {attempt+1}: {e}\")\n",
        "\n",
        "    # If all retries fail, generate a more varied story using a different approach\n",
        "    print(\"All generation attempts failed. Using alternative generation method...\")\n",
        "    return generate_alternative_story(prompt)\n",
        "\n",
        "def generate_alternative_story(prompt):\n",
        "    \"\"\"\n",
        "    Generate a more varied story when the model-based generation fails.\n",
        "    This creates a more dynamic story than the previous structured approach.\n",
        "    \"\"\"\n",
        "    character = extract_character_name(prompt) or \"the little hero\"\n",
        "    topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "    # Create a list of possible story templates\n",
        "    story_templates = [\n",
        "        f\"\"\"Once upon a time, there was {character} who lived in a small village.\n",
        "{character} loved {topic} more than anything else.\n",
        "\n",
        "One day, {character} discovered something magical while playing outside. It was a tiny glowing ball that could float in the air.\n",
        "\n",
        "\"What's this?\" wondered {character}. The ball started to move, and {character} followed it into the forest.\n",
        "\n",
        "The ball led {character} to a hidden garden where other children were playing with similar magical balls. They welcomed {character} and taught them new games.\n",
        "\n",
        "{character} had so much fun that day! When it was time to go home, one of the children gave {character} a magical ball of their own.\n",
        "\n",
        "From that day on, {character} visited the hidden garden often and made many new friends. They all loved {topic} together and had wonderful adventures.\"\"\",\n",
        "\n",
        "        f\"\"\"{character} woke up early one morning with a big smile. Today was the day for {topic}!\n",
        "\n",
        "{character} packed a small bag with snacks and a water bottle. \"I'm ready for my adventure!\" {character} said.\n",
        "\n",
        "Outside, {character} met a friendly cat. \"Hello, cat! Would you like to join me for {topic}?\" The cat meowed and followed along.\n",
        "\n",
        "As they walked through the park, they found a lost teddy bear. \"We should find who this belongs to,\" said {character}.\n",
        "\n",
        "They asked everyone they met about the teddy bear. Finally, they met a little girl who was crying. \"My teddy!\" she said happily when she saw it.\n",
        "\n",
        "\"Thank you for helping me find my teddy,\" the girl smiled. \"Would you like to play with me?\"\n",
        "\n",
        "{character}, the cat, and the girl spent the whole day playing and having fun with {topic}. It was the best day ever!\"\"\",\n",
        "\n",
        "        f\"\"\"The sun was shining brightly when {character} decided it was the perfect day for {topic}.\n",
        "\n",
        "\"I've never tried {topic} before,\" {character} thought, \"but today I will be brave!\"\n",
        "\n",
        "{character} began slowly, taking small steps. It wasn't easy at first, and {character} almost gave up.\n",
        "\n",
        "\"Don't worry, you can do it!\" said a friendly voice. {character} looked up and saw a smiling butterfly.\n",
        "\n",
        "With the butterfly's encouragement, {character} tried again. This time, {character} did much better!\n",
        "\n",
        "Soon, {character} was having so much fun with {topic}. \"Thank you for believing in me,\" {character} said to the butterfly.\n",
        "\n",
        "The butterfly fluttered around happily. \"Sometimes we just need a friend to help us be brave,\" it replied.\n",
        "\n",
        "{character} went home that evening with a happy heart, excited to try {topic} again tomorrow.\"\"\"\n",
        "    ]\n",
        "\n",
        "    import random\n",
        "    return random.choice(story_templates)\n",
        "\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        if len(paragraphs) > num_panels:\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    sentences = []\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]\n",
        "\n",
        "    if len(sentences) < num_panels:\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        # For the last panel, include any remaining sentences\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        # Return the first dialogue found\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        # Load the model - adjust the model_id based on what base model your safetensors file is trained on\n",
        "        # Assuming it's SD 1.5 based; modify if it's based on a different architecture\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        # Load the custom weights\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        # Check if CUDA is available, otherwise use CPU\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "# Original SDXL model load function as fallback\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available, otherwise use CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Function to add speech bubbles to images\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    # Convert PIL image to numpy array for OpenCV operations\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    # If position not provided, place the bubble at a suitable position\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    # Create a transparent layer for the bubble\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    # Calculate bubble size based on text length\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    # Draw speech bubble\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    # Add a pointer to make it look like a speech bubble\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    # Draw text\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    # Try to load a font, fall back to default if not available\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    # Wrap text to fit in the bubble\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    # Calculate text position for centering\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    # Draw the wrapped text\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    # Combine the original image with the bubble\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "# Function to wrap text to fit within a certain width\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        # Test if adding this word would exceed the width\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            # Use getbbox or getlength instead of deprecated getsize\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]  # right - left\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]  # Fallback to deprecated method\n",
        "        except:\n",
        "            # If all methods fail, estimate based on character count\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    # Add the last line\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "# NEW FEATURE: Predefined comic templates\n",
        "def get_template_data(template_name):\n",
        "    \"\"\"Return predefined prompts and dialogue for various comic templates\"\"\"\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)],\n",
        "            \"story\": \"The night was cold in Gotham City. Crime was at an all-time high, and the police were overwhelmed. Standing on the tallest building in the city, Batman observed the dark streets below. The Bat-Signal illuminated the night sky, calling for his help.\\n\\nBatman knew his duty to the city. With unwavering resolve, he prepared himself for another night of fighting crime. His years of training had made him not just a vigilante, but a symbol of justice.\\n\\nSpotting a group of armed criminals attempting to rob a bank, Batman leapt from the rooftop. His cape spread like wings as he descended through the night air, becoming the fear that criminals dreaded.\\n\\nWithin moments, Batman had subdued the criminals. Another night in Gotham, another battle won. But Batman knew his work would never be done as long as there was crime in his city. He was the Dark Knight, Gotham's silent guardian.\"\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)],\n",
        "            \"story\": \"Peter Parker sat perched on the edge of a skyscraper, his Spider-Man mask pulled up just enough to enjoy a quick sandwich. The skyline of New York City stretched out before him, beautiful even in the chaos of the busy afternoon. He reflected on his Uncle Ben's words that had become his guiding principle.\\n\\nFinishing his lunch, Spider-Man pulled down his mask and leapt off the building, shooting a web that attached to a nearby tower. He swung through the concrete canyons of Manhattan with incredible agility, waving to surprised office workers as he passed their windows.\\n\\nSuddenly, his spider-sense warned him of danger. Several blocks away, the Rhino was tearing through cars and causing chaos. Spider-Man swung towards the trouble, landing in a fighting stance before the villain, ready to protect the innocent people fleeing the scene.\\n\\nAfter subduing Rhino with his webs, Spider-Man helped a scared young child find his mother among the crowd. The boy smiled up at his hero, and Spider-Man remembered why he did this every day. Being a hero wasn't about the glory—it was about helping people, one person at a time.\"\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)],\n",
        "            \"story\": \"The sun rose over Themyscira, illuminating the peaceful island paradise of the Amazons. Diana, princess of the Amazons, stood on a cliff overlooking the sea, her armor gleaming in the morning light. She had been trained as a warrior since birth, but her mission now extended beyond her home.\\n\\nThat mission brought her to the world of mankind, where conflict raged. In the midst of a battle, Diana stood firm against a hail of bullets, deflecting them with her enchanted bracelets. Her years of training had prepared her for this moment—to protect those who could not protect themselves.\\n\\nLater, Diana found herself facing ancient monsters awakened from myth, creatures that threatened both gods and humans alike. With sword and shield in hand, she engaged in combat, her movements a blur of strength and grace, demonstrating that true power comes from fighting for what is right.\\n\\nAfter the battle, Diana used her lasso of truth to extract critical information from an enemy agent. The golden rope glowed with divine energy as she wielded it with compassion and wisdom. Diana knew that while force might win battles, truth and love would ultimately win the war.\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "        story_text = template_data[\"story\"]\n",
        "\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        # If the story needs to be trimmed based on the number of panels requested\n",
        "        if num_panels < 4:\n",
        "            story_paragraphs = story_text.split('\\n\\n')\n",
        "            story_text = '\\n\\n'.join(story_paragraphs[:num_panels])\n",
        "\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        # Split the story into panels based on paragraphs\n",
        "        story_panels = story_text.split('\\n\\n')[:num_panels]\n",
        "\n",
        "        # If we have fewer paragraphs than panels, duplicate the last paragraph\n",
        "        while len(story_panels) < num_panels:\n",
        "            story_panels.append(story_panels[-1])\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # If story generation still produces insufficient content after multiple attempts\n",
        "        # and with different parameters, try generating with a completely different approach\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Using alternative generation approach.\")\n",
        "            story_text = generate_alternative_story(story_prompt)\n",
        "            print(\"Alternative story generated:\")\n",
        "            print(story_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in model-based story generation: {e}\")\n",
        "        print(\"Using alternative story generation approach...\")\n",
        "        story_text = generate_alternative_story(story_prompt)\n",
        "\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Make the image prompts unique for each panel based on the actual story content\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                position = (256, 100 + (i % 2) * 50)\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # Add template selection dropdown\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),\n",
        "                    label=\"Choose Template (optional)\",\n",
        "                    value=\"None\"\n",
        "                )\n",
        "\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to fly')\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                num_panels = gr.Slider(\n",
        "                    label=\"Number of Panels\",\n",
        "                    minimum=2,\n",
        "                    maximum=8,\n",
        "                    value=4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "VVJJb9dq3Oy3",
        "outputId": "be5e275e-b711-43b8-fce8-692cf64004e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-9-db059e05f110>, line 719)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-db059e05f110>\"\u001b[0;36m, line \u001b[0;32m719\u001b[0m\n\u001b[0;31m    value=4\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Try loading a fallback model if the fine-tuned one fails\n",
        "        try:\n",
        "            print(\"Attempting to load fallback model gpt2...\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = model.to(device)\n",
        "            print(f\"Fallback model loaded successfully and moved to {device}\")\n",
        "            return model, tokenizer\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading fallback model: {e2}\")\n",
        "            return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_story(model, tokenizer, prompt, max_length=400, retries=3):\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    # Increase repetition penalty and adjust parameters for better generation\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "            device = next(model.parameters()).device\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Use different generation parameters for each retry attempt\n",
        "            temperature = 0.7 + (attempt * 0.1)  # Gradually increase temperature with each retry\n",
        "            repetition_penalty = 1.2 + (attempt * 0.1)  # Gradually increase repetition penalty\n",
        "\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=3,  # Increased from 2\n",
        "                top_k=50,  # Increased from 40\n",
        "                top_p=0.92,  # Adjusted from 0.9\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=repetition_penalty\n",
        "            )\n",
        "\n",
        "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Clean up the generated text to focus on the actual story\n",
        "            if story_prompt in generated_text:\n",
        "                generated_text = generated_text.replace(story_prompt, \"\").strip()\n",
        "\n",
        "            # Check if we have a reasonable story\n",
        "            if len(generated_text) > 100 and \".\" in generated_text:\n",
        "                print(f\"Successfully generated story on attempt {attempt+1}\")\n",
        "                return generated_text\n",
        "            else:\n",
        "                print(f\"Attempt {attempt+1} produced insufficient content ({len(generated_text)} chars). Retrying...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in story generation attempt {attempt+1}: {e}\")\n",
        "\n",
        "    # If all retries fail, generate a more varied story using a different approach\n",
        "    print(\"All generation attempts failed. Using alternative generation method...\")\n",
        "    return generate_alternative_story(prompt)\n",
        "\n",
        "def generate_alternative_story(prompt):\n",
        "    \"\"\"\n",
        "    Generate a more varied story when the model-based generation fails.\n",
        "    This creates a more dynamic story than the previous structured approach.\n",
        "    \"\"\"\n",
        "    character = extract_character_name(prompt) or \"the little hero\"\n",
        "    topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "    # Create a list of possible story templates\n",
        "    story_templates = [\n",
        "        f\"\"\"Once upon a time, there was {character} who lived in a small village.\n",
        "{character} loved {topic} more than anything else.\n",
        "\n",
        "One day, {character} discovered something magical while playing outside. It was a tiny glowing ball that could float in the air.\n",
        "\n",
        "\"What's this?\" wondered {character}. The ball started to move, and {character} followed it into the forest.\n",
        "\n",
        "The ball led {character} to a hidden garden where other children were playing with similar magical balls. They welcomed {character} and taught them new games.\n",
        "\n",
        "{character} had so much fun that day! When it was time to go home, one of the children gave {character} a magical ball of their own.\n",
        "\n",
        "From that day on, {character} visited the hidden garden often and made many new friends. They all loved {topic} together and had wonderful adventures.\"\"\",\n",
        "\n",
        "        f\"\"\"{character} woke up early one morning with a big smile. Today was the day for {topic}!\n",
        "\n",
        "{character} packed a small bag with snacks and a water bottle. \"I'm ready for my adventure!\" {character} said.\n",
        "\n",
        "Outside, {character} met a friendly cat. \"Hello, cat! Would you like to join me for {topic}?\" The cat meowed and followed along.\n",
        "\n",
        "As they walked through the park, they found a lost teddy bear. \"We should find who this belongs to,\" said {character}.\n",
        "\n",
        "They asked everyone they met about the teddy bear. Finally, they met a little girl who was crying. \"My teddy!\" she said happily when she saw it.\n",
        "\n",
        "\"Thank you for helping me find my teddy,\" the girl smiled. \"Would you like to play with me?\"\n",
        "\n",
        "{character}, the cat, and the girl spent the whole day playing and having fun with {topic}. It was the best day ever!\"\"\",\n",
        "\n",
        "        f\"\"\"The sun was shining brightly when {character} decided it was the perfect day for {topic}.\n",
        "\n",
        "\"I've never tried {topic} before,\" {character} thought, \"but today I will be brave!\"\n",
        "\n",
        "{character} began slowly, taking small steps. It wasn't easy at first, and {character} almost gave up.\n",
        "\n",
        "\"Don't worry, you can do it!\" said a friendly voice. {character} looked up and saw a smiling butterfly.\n",
        "\n",
        "With the butterfly's encouragement, {character} tried again. This time, {character} did much better!\n",
        "\n",
        "Soon, {character} was having so much fun with {topic}. \"Thank you for believing in me,\" {character} said to the butterfly.\n",
        "\n",
        "The butterfly fluttered around happily. \"Sometimes we just need a friend to help us be brave,\" it replied.\n",
        "\n",
        "{character} went home that evening with a happy heart, excited to try {topic} again tomorrow.\"\"\"\n",
        "    ]\n",
        "\n",
        "    import random\n",
        "    return random.choice(story_templates)\n",
        "\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        if len(paragraphs) > num_panels:\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    sentences = []\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]\n",
        "\n",
        "    if len(sentences) < num_panels:\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        # For the last panel, include any remaining sentences\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        # Return the first dialogue found\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        # Load the model - adjust the model_id based on what base model your safetensors file is trained on\n",
        "        # Assuming it's SD 1.5 based; modify if it's based on a different architecture\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        # Load the custom weights\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        # Check if CUDA is available, otherwise use CPU\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "# Original SDXL model load function as fallback\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available, otherwise use CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Function to add speech bubbles to images\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    # Convert PIL image to numpy array for OpenCV operations\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    # If position not provided, place the bubble at a suitable position\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    # Create a transparent layer for the bubble\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    # Calculate bubble size based on text length\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    # Draw speech bubble\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    # Add a pointer to make it look like a speech bubble\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    # Draw text\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    # Try to load a font, fall back to default if not available\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    # Wrap text to fit in the bubble\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    # Calculate text position for centering\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    # Draw the wrapped text\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    # Combine the original image with the bubble\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "# Function to wrap text to fit within a certain width\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        # Test if adding this word would exceed the width\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            # Use getbbox or getlength instead of deprecated getsize\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]  # right - left\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]  # Fallback to deprecated method\n",
        "        except:\n",
        "            # If all methods fail, estimate based on character count\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    # Add the last line\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "# NEW FEATURE: Predefined comic templates\n",
        "def get_template_data(template_name):\n",
        "    \"\"\"Return predefined prompts and dialogue for various comic templates\"\"\"\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)],\n",
        "            \"story\": \"The night was cold in Gotham City. Crime was at an all-time high, and the police were overwhelmed. Standing on the tallest building in the city, Batman observed the dark streets below. The Bat-Signal illuminated the night sky, calling for his help.\\n\\nBatman knew his duty to the city. With unwavering resolve, he prepared himself for another night of fighting crime. His years of training had made him not just a vigilante, but a symbol of justice.\\n\\nSpotting a group of armed criminals attempting to rob a bank, Batman leapt from the rooftop. His cape spread like wings as he descended through the night air, becoming the fear that criminals dreaded.\\n\\nWithin moments, Batman had subdued the criminals. Another night in Gotham, another battle won. But Batman knew his work would never be done as long as there was crime in his city. He was the Dark Knight, Gotham's silent guardian.\"\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)],\n",
        "            \"story\": \"Peter Parker sat perched on the edge of a skyscraper, his Spider-Man mask pulled up just enough to enjoy a quick sandwich. The skyline of New York City stretched out before him, beautiful even in the chaos of the busy afternoon. He reflected on his Uncle Ben's words that had become his guiding principle.\\n\\nFinishing his lunch, Spider-Man pulled down his mask and leapt off the building, shooting a web that attached to a nearby tower. He swung through the concrete canyons of Manhattan with incredible agility, waving to surprised office workers as he passed their windows.\\n\\nSuddenly, his spider-sense warned him of danger. Several blocks away, the Rhino was tearing through cars and causing chaos. Spider-Man swung towards the trouble, landing in a fighting stance before the villain, ready to protect the innocent people fleeing the scene.\\n\\nAfter subduing Rhino with his webs, Spider-Man helped a scared young child find his mother among the crowd. The boy smiled up at his hero, and Spider-Man remembered why he did this every day. Being a hero wasn't about the glory—it was about helping people, one person at a time.\"\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)],\n",
        "            \"story\": \"The sun rose over Themyscira, illuminating the peaceful island paradise of the Amazons. Diana, princess of the Amazons, stood on a cliff overlooking the sea, her armor gleaming in the morning light. She had been trained as a warrior since birth, but her mission now extended beyond her home.\\n\\nThat mission brought her to the world of mankind, where conflict raged. In the midst of a battle, Diana stood firm against a hail of bullets, deflecting them with her enchanted bracelets. Her years of training had prepared her for this moment—to protect those who could not protect themselves.\\n\\nLater, Diana found herself facing ancient monsters awakened from myth, creatures that threatened both gods and humans alike. With sword and shield in hand, she engaged in combat, her movements a blur of strength and grace, demonstrating that true power comes from fighting for what is right.\\n\\nAfter the battle, Diana used her lasso of truth to extract critical information from an enemy agent. The golden rope glowed with divine energy as she wielded it with compassion and wisdom. Diana knew that while force might win battles, truth and love would ultimately win the war.\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "        story_text = template_data[\"story\"]\n",
        "\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        # If the story needs to be trimmed based on the number of panels requested\n",
        "        if num_panels < 4:\n",
        "            story_paragraphs = story_text.split('\\n\\n')\n",
        "            story_text = '\\n\\n'.join(story_paragraphs[:num_panels])\n",
        "\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        # Split the story into panels based on paragraphs\n",
        "        story_panels = story_text.split('\\n\\n')[:num_panels]\n",
        "\n",
        "        # If we have fewer paragraphs than panels, duplicate the last paragraph\n",
        "        while len(story_panels) < num_panels:\n",
        "            story_panels.append(story_panels[-1])\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # If story generation still produces insufficient content after multiple attempts\n",
        "        # and with different parameters, try generating with a completely different approach\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Using alternative generation approach.\")\n",
        "            story_text = generate_alternative_story(story_prompt)\n",
        "            print(\"Alternative story generated:\")\n",
        "            print(story_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in model-based story generation: {e}\")\n",
        "        print(\"Using alternative story generation approach...\")\n",
        "        story_text = generate_alternative_story(story_prompt)\n",
        "\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Make the image prompts unique for each panel based on the actual story content\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                position = (256, 100 + (i % 2) * 50)\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # Add template selection dropdown\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),\n",
        "                    label=\"Choose Template (optional)\",\n",
        "                    value=\"None\"\n",
        "                )\n",
        "\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "KhWCA7Ao9-5b",
        "outputId": "fff32645-561c-490a-c874-eecc00bee2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 711) (<ipython-input-10-a99ce5ee10c1>, line 711)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-a99ce5ee10c1>\"\u001b[0;36m, line \u001b[0;32m711\u001b[0m\n\u001b[0;31m    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 711)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Try loading a fallback model if the fine-tuned one fails\n",
        "        try:\n",
        "            print(\"Attempting to load fallback model gpt2...\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = model.to(device)\n",
        "            print(f\"Fallback model loaded successfully and moved to {device}\")\n",
        "            return model, tokenizer\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading fallback model: {e2}\")\n",
        "            return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_story(model, tokenizer, prompt, max_length=400, retries=3):\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    # Increase repetition penalty and adjust parameters for better generation\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "            device = next(model.parameters()).device\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Use different generation parameters for each retry attempt\n",
        "            temperature = 0.7 + (attempt * 0.1)  # Gradually increase temperature with each retry\n",
        "            repetition_penalty = 1.2 + (attempt * 0.1)  # Gradually increase repetition penalty\n",
        "\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=3,  # Increased from 2\n",
        "                top_k=50,  # Increased from 40\n",
        "                top_p=0.92,  # Adjusted from 0.9\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=repetition_penalty\n",
        "            )\n",
        "\n",
        "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Clean up the generated text to focus on the actual story\n",
        "            if story_prompt in generated_text:\n",
        "                generated_text = generated_text.replace(story_prompt, \"\").strip()\n",
        "\n",
        "            # Check if we have a reasonable story\n",
        "            if len(generated_text) > 100 and \".\" in generated_text:\n",
        "                print(f\"Successfully generated story on attempt {attempt+1}\")\n",
        "                return generated_text\n",
        "            else:\n",
        "                print(f\"Attempt {attempt+1} produced insufficient content ({len(generated_text)} chars). Retrying...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in story generation attempt {attempt+1}: {e}\")\n",
        "\n",
        "    # If all retries fail, generate a more varied story using a different approach\n",
        "    print(\"All generation attempts failed. Using alternative generation method...\")\n",
        "    return generate_alternative_story(prompt)\n",
        "\n",
        "def generate_alternative_story(prompt):\n",
        "    \"\"\"\n",
        "    Generate a more varied story when the model-based generation fails.\n",
        "    This creates a more dynamic story than the previous structured approach.\n",
        "    \"\"\"\n",
        "    character = extract_character_name(prompt) or \"the little hero\"\n",
        "    topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "    # Create a list of possible story templates\n",
        "    story_templates = [\n",
        "        f\"\"\"Once upon a time, there was {character} who lived in a small village.\n",
        "{character} loved {topic} more than anything else.\n",
        "\n",
        "One day, {character} discovered something magical while playing outside. It was a tiny glowing ball that could float in the air.\n",
        "\n",
        "\"What's this?\" wondered {character}. The ball started to move, and {character} followed it into the forest.\n",
        "\n",
        "The ball led {character} to a hidden garden where other children were playing with similar magical balls. They welcomed {character} and taught them new games.\n",
        "\n",
        "{character} had so much fun that day! When it was time to go home, one of the children gave {character} a magical ball of their own.\n",
        "\n",
        "From that day on, {character} visited the hidden garden often and made many new friends. They all loved {topic} together and had wonderful adventures.\"\"\",\n",
        "\n",
        "        f\"\"\"{character} woke up early one morning with a big smile. Today was the day for {topic}!\n",
        "\n",
        "{character} packed a small bag with snacks and a water bottle. \"I'm ready for my adventure!\" {character} said.\n",
        "\n",
        "Outside, {character} met a friendly cat. \"Hello, cat! Would you like to join me for {topic}?\" The cat meowed and followed along.\n",
        "\n",
        "As they walked through the park, they found a lost teddy bear. \"We should find who this belongs to,\" said {character}.\n",
        "\n",
        "They asked everyone they met about the teddy bear. Finally, they met a little girl who was crying. \"My teddy!\" she said happily when she saw it.\n",
        "\n",
        "\"Thank you for helping me find my teddy,\" the girl smiled. \"Would you like to play with me?\"\n",
        "\n",
        "{character}, the cat, and the girl spent the whole day playing and having fun with {topic}. It was the best day ever!\"\"\",\n",
        "\n",
        "        f\"\"\"The sun was shining brightly when {character} decided it was the perfect day for {topic}.\n",
        "\n",
        "\"I've never tried {topic} before,\" {character} thought, \"but today I will be brave!\"\n",
        "\n",
        "{character} began slowly, taking small steps. It wasn't easy at first, and {character} almost gave up.\n",
        "\n",
        "\"Don't worry, you can do it!\" said a friendly voice. {character} looked up and saw a smiling butterfly.\n",
        "\n",
        "With the butterfly's encouragement, {character} tried again. This time, {character} did much better!\n",
        "\n",
        "Soon, {character} was having so much fun with {topic}. \"Thank you for believing in me,\" {character} said to the butterfly.\n",
        "\n",
        "The butterfly fluttered around happily. \"Sometimes we just need a friend to help us be brave,\" it replied.\n",
        "\n",
        "{character} went home that evening with a happy heart, excited to try {topic} again tomorrow.\"\"\"\n",
        "    ]\n",
        "\n",
        "    import random\n",
        "    return random.choice(story_templates)\n",
        "\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        if len(paragraphs) > num_panels:\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    sentences = []\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]\n",
        "\n",
        "    if len(sentences) < num_panels:\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]\n",
        "        except:\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "def get_template_data(template_name):\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)],\n",
        "            \"story\": \"\"\"The Dark Knight watches over Gotham City from high above. Tonight is like any other night for Batman - a city full of shadows that need his protection.\n",
        "\n",
        "As the Bat-Signal illuminates the cloudy sky, Batman's eyes narrow with determination. The signal isn't just a call for help - it's a reminder of the oath he took to protect the innocent.\n",
        "\n",
        "Batman spots a group of criminals attempting to rob a local shop. He leaps from the rooftop, his cape billowing around him like massive wings. The criminals look up in terror as his silhouette blots out the moonlight.\n",
        "\n",
        "With swift, calculated movements, Batman takes down the criminals one by one. Another night in Gotham, another battle won. But Batman knows his work is never truly done. As long as there is crime in Gotham, he will be there to fight it.\"\"\"\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)],\n",
        "            \"story\": \"\"\"Peter Parker sits perched on the edge of a towering skyscraper, overlooking the bustling streets of New York City. His uncle Ben's words echo in his mind as he surveys the city he's sworn to protect.\n",
        "\n",
        "With a flick of his wrist, Spider-Man launches himself off the building, shooting a web that attaches to a nearby structure. He swings gracefully through the concrete canyons of Manhattan, the wind rushing past his mask as pedestrians below point and cheer.\n",
        "\n",
        "Suddenly, his spider-sense alerts him to danger. A bank robbery is in progress just a few blocks away. Spider-Man changes direction mid-swing, heading straight for the trouble. As he arrives, he sees masked robbers threatening innocent civilians.\n",
        "\n",
        "With swift movements and well-placed webs, Spider-Man subdues the criminals and ensures everyone is safe. A young child approaches him with wide eyes of admiration. Spider-Man kneels down to their level, reminding them that anyone can be a hero if they choose to help others.\"\"\"\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)],\n",
        "            \"story\": \"\"\"The sun rises over Themyscira, casting a golden glow on the island paradise. Diana stands proudly on a cliff overlooking the ocean, her armor gleaming in the morning light, ready to face whatever challenges the day may bring.\n",
        "\n",
        "News reaches the Amazon princess of danger in the world of men. Without hesitation, Diana leaves her homeland behind. In the midst of chaos, Wonder Woman stands firm, her bracelets deflecting a barrage of bullets meant to harm innocent bystanders.\n",
        "\n",
        "Ancient mythological creatures, released by a power-hungry sorcerer, terrorize a small village. Wonder Woman meets them in battle, her sword and shield moving with incredible speed and precision. Her training as an Amazon warrior guides each strike.\n",
        "\n",
        "With the creatures defeated, Wonder Woman confronts the sorcerer. She wraps her golden Lasso of Truth around him, the mystical rope glowing with divine energy. As the magic of the lasso compels him to speak honestly, the truth behind his actions is revealed, and justice can finally be served.\"\"\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        # Use the pre-written story from the template data\n",
        "        story_text = template_data[\"story\"]\n",
        "        story_panels = prompts\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # If story generation still produces insufficient content after multiple attempts\n",
        "        # and with different parameters, try generating with a completely different approach\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Using alternative generation approach.\")\n",
        "            story_text = generate_alternative_story(story_prompt)\n",
        "            print(\"Alternative story generated:\")\n",
        "            print(story_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in model-based story generation: {e}\")\n",
        "        print(\"Using alternative story generation approach...\")\n",
        "        story_text = generate_alternative_story(story_prompt)\n",
        "\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Make the image prompts unique for each panel based on the actual story content\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                position = (256, 100 + (i % 2) * 50)\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),\n",
        "                    label=\"Choose Template (optional)\",\n",
        "                    value=\"None\"\n",
        "                )\n",
        "\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to fly')\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                num_panels = gr.Slider(\n",
        "                    label=\"Number of Panels\",\n",
        "                    minimum=2,\n",
        "                    maximum=8,\n",
        "                    value=4,\n",
        "                    step=1\n",
        "                )\n",
        "art_style = gr.Dropdown(\n",
        "                    choices=[\"Cartoon Style\", \"Realistic\", \"Anime\", \"Sketch\"],\n",
        "                    label=\"Art Style\",\n",
        "                    value=\"Cartoon Style\"\n",
        "                )\n",
        "\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"Generated Story\",\n",
        "                    placeholder=\"The generated story will appear here...\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "                 output_panels = gr.Gallery(\n",
        "                    label=\"Comic Panels\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"comic-panels\",\n",
        "                    height=200,\n",
        "                    columns=2  # Set the number of columns for the gallery\n",
        "                )\n",
        "\n",
        "                output_comic = gr.Image(\n",
        "                    label=\"Combined Comic Strip\",\n",
        "                    type=\"pil\"\n",
        "                )\n",
        "\n",
        "        # Fixed function to handle the generation and properly return outputs\n",
        "        def generate_comic_action(story_prompt, num_panels, art_style, use_template):\n",
        "            story_text, story_panels, panel_images, comic_strip = generate_comic(\n",
        "                story_prompt, num_panels, art_style=art_style, use_template=use_template\n",
        "            )\n",
        "# Return the outputs directly rather than updating them\n",
        "            return story_text, panel_images if panel_images else [], comic_strip\n",
        "\n",
        "        generate_button.click(\n",
        "            generate_comic_action,\n",
        "            inputs=[story_prompt, num_panels, art_style, template_dropdown],\n",
        "            outputs=[output_text, output_panels, output_comic]\n",
        "        )\n",
        "\n",
        "    app.launch()\n",
        "\n",
        "# Mount Google Drive if in Colab environment\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Not running in Colab or drive already mounted\")\n",
        "\n",
        "# Call the function to create the UI\n",
        "if name == \"main\":\n",
        "    create_ui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Q9fk-d2x-2qw",
        "outputId": "10774ca3-6480-4ed5-fc76-68b25d0d0abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 710)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m710\u001b[0m\n\u001b[0;31m    with gr.Column():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Try loading a fallback model if the fine-tuned one fails\n",
        "        try:\n",
        "            print(\"Attempting to load fallback model gpt2...\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = model.to(device)\n",
        "            print(f\"Fallback model loaded successfully and moved to {device}\")\n",
        "            return model, tokenizer\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading fallback model: {e2}\")\n",
        "            return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_story(model, tokenizer, prompt, max_length=400, retries=3):\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    # Increase repetition penalty and adjust parameters for better generation\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "            device = next(model.parameters()).device\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Use different generation parameters for each retry attempt\n",
        "            temperature = 0.7 + (attempt * 0.1)  # Gradually increase temperature with each retry\n",
        "            repetition_penalty = 1.2 + (attempt * 0.1)  # Gradually increase repetition penalty\n",
        "\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=3,  # Increased from 2\n",
        "                top_k=50,  # Increased from 40\n",
        "                top_p=0.92,  # Adjusted from 0.9\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=repetition_penalty\n",
        "            )\n",
        "\n",
        "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Clean up the generated text to focus on the actual story\n",
        "            if story_prompt in generated_text:\n",
        "                generated_text = generated_text.replace(story_prompt, \"\").strip()\n",
        "\n",
        "            # Check if we have a reasonable story\n",
        "            if len(generated_text) > 100 and \".\" in generated_text:\n",
        "                print(f\"Successfully generated story on attempt {attempt+1}\")\n",
        "                return generated_text\n",
        "            else:\n",
        "                print(f\"Attempt {attempt+1} produced insufficient content ({len(generated_text)} chars). Retrying...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in story generation attempt {attempt+1}: {e}\")\n",
        "\n",
        "    # If all retries fail, generate a more varied story using a different approach\n",
        "    print(\"All generation attempts failed. Using alternative generation method...\")\n",
        "    return generate_alternative_story(prompt)\n",
        "\n",
        "def generate_alternative_story(prompt):\n",
        "    \"\"\"\n",
        "    Generate a more varied story when the model-based generation fails.\n",
        "    This creates a more dynamic story than the previous structured approach.\n",
        "    \"\"\"\n",
        "    character = extract_character_name(prompt) or \"the little hero\"\n",
        "    topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "    # Create a list of possible story templates\n",
        "    story_templates = [\n",
        "        f\"\"\"Once upon a time, there was {character} who lived in a small village.\n",
        "{character} loved {topic} more than anything else.\n",
        "\n",
        "One day, {character} discovered something magical while playing outside. It was a tiny glowing ball that could float in the air.\n",
        "\n",
        "\"What's this?\" wondered {character}. The ball started to move, and {character} followed it into the forest.\n",
        "\n",
        "The ball led {character} to a hidden garden where other children were playing with similar magical balls. They welcomed {character} and taught them new games.\n",
        "\n",
        "{character} had so much fun that day! When it was time to go home, one of the children gave {character} a magical ball of their own.\n",
        "\n",
        "From that day on, {character} visited the hidden garden often and made many new friends. They all loved {topic} together and had wonderful adventures.\"\"\",\n",
        "\n",
        "        f\"\"\"{character} woke up early one morning with a big smile. Today was the day for {topic}!\n",
        "\n",
        "{character} packed a small bag with snacks and a water bottle. \"I'm ready for my adventure!\" {character} said.\n",
        "\n",
        "Outside, {character} met a friendly cat. \"Hello, cat! Would you like to join me for {topic}?\" The cat meowed and followed along.\n",
        "\n",
        "As they walked through the park, they found a lost teddy bear. \"We should find who this belongs to,\" said {character}.\n",
        "\n",
        "They asked everyone they met about the teddy bear. Finally, they met a little girl who was crying. \"My teddy!\" she said happily when she saw it.\n",
        "\n",
        "\"Thank you for helping me find my teddy,\" the girl smiled. \"Would you like to play with me?\"\n",
        "\n",
        "{character}, the cat, and the girl spent the whole day playing and having fun with {topic}. It was the best day ever!\"\"\",\n",
        "\n",
        "        f\"\"\"The sun was shining brightly when {character} decided it was the perfect day for {topic}.\n",
        "\n",
        "\"I've never tried {topic} before,\" {character} thought, \"but today I will be brave!\"\n",
        "\n",
        "{character} began slowly, taking small steps. It wasn't easy at first, and {character} almost gave up.\n",
        "\n",
        "\"Don't worry, you can do it!\" said a friendly voice. {character} looked up and saw a smiling butterfly.\n",
        "\n",
        "With the butterfly's encouragement, {character} tried again. This time, {character} did much better!\n",
        "\n",
        "Soon, {character} was having so much fun with {topic}. \"Thank you for believing in me,\" {character} said to the butterfly.\n",
        "\n",
        "The butterfly fluttered around happily. \"Sometimes we just need a friend to help us be brave,\" it replied.\n",
        "\n",
        "{character} went home that evening with a happy heart, excited to try {topic} again tomorrow.\"\"\"\n",
        "    ]\n",
        "\n",
        "    import random\n",
        "    return random.choice(story_templates)\n",
        "\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        if len(paragraphs) > num_panels:\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    sentences = []\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]\n",
        "\n",
        "    if len(sentences) < num_panels:\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]\n",
        "        except:\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "def get_template_data(template_name):\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)],\n",
        "            \"story\": \"\"\"The Dark Knight watches over Gotham City from high above. Tonight is like any other night for Batman - a city full of shadows that need his protection.\n",
        "\n",
        "As the Bat-Signal illuminates the cloudy sky, Batman's eyes narrow with determination. The signal isn't just a call for help - it's a reminder of the oath he took to protect the innocent.\n",
        "\n",
        "Batman spots a group of criminals attempting to rob a local shop. He leaps from the rooftop, his cape billowing around him like massive wings. The criminals look up in terror as his silhouette blots out the moonlight.\n",
        "\n",
        "With swift, calculated movements, Batman takes down the criminals one by one. Another night in Gotham, another battle won. But Batman knows his work is never truly done. As long as there is crime in Gotham, he will be there to fight it.\"\"\"\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)],\n",
        "            \"story\": \"\"\"Peter Parker sits perched on the edge of a towering skyscraper, overlooking the bustling streets of New York City. His uncle Ben's words echo in his mind as he surveys the city he's sworn to protect.\n",
        "\n",
        "With a flick of his wrist, Spider-Man launches himself off the building, shooting a web that attaches to a nearby structure. He swings gracefully through the concrete canyons of Manhattan, the wind rushing past his mask as pedestrians below point and cheer.\n",
        "\n",
        "Suddenly, his spider-sense alerts him to danger. A bank robbery is in progress just a few blocks away. Spider-Man changes direction mid-swing, heading straight for the trouble. As he arrives, he sees masked robbers threatening innocent civilians.\n",
        "\n",
        "With swift movements and well-placed webs, Spider-Man subdues the criminals and ensures everyone is safe. A young child approaches him with wide eyes of admiration. Spider-Man kneels down to their level, reminding them that anyone can be a hero if they choose to help others.\"\"\"\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)],\n",
        "            \"story\": \"\"\"The sun rises over Themyscira, casting a golden glow on the island paradise. Diana stands proudly on a cliff overlooking the ocean, her armor gleaming in the morning light, ready to face whatever challenges the day may bring.\n",
        "\n",
        "News reaches the Amazon princess of danger in the world of men. Without hesitation, Diana leaves her homeland behind. In the midst of chaos, Wonder Woman stands firm, her bracelets deflecting a barrage of bullets meant to harm innocent bystanders.\n",
        "\n",
        "Ancient mythological creatures, released by a power-hungry sorcerer, terrorize a small village. Wonder Woman meets them in battle, her sword and shield moving with incredible speed and precision. Her training as an Amazon warrior guides each strike.\n",
        "\n",
        "With the creatures defeated, Wonder Woman confronts the sorcerer. She wraps her golden Lasso of Truth around him, the mystical rope glowing with divine energy. As the magic of the lasso compels him to speak honestly, the truth behind his actions is revealed, and justice can finally be served.\"\"\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        # Use the pre-written story from the template data\n",
        "        story_text = template_data[\"story\"]\n",
        "        story_panels = prompts\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # If story generation still produces insufficient content after multiple attempts\n",
        "        # and with different parameters, try generating with a completely different approach\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Using alternative generation approach.\")\n",
        "            story_text = generate_alternative_story(story_prompt)\n",
        "            print(\"Alternative story generated:\")\n",
        "            print(story_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in model-based story generation: {e}\")\n",
        "        print(\"Using alternative story generation approach...\")\n",
        "        story_text = generate_alternative_story(story_prompt)\n",
        "\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Make the image prompts unique for each panel based on the actual story content\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                position = (256, 100 + (i % 2) * 50)\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),\n",
        "                    label=\"Choose Template (optional)\",\n",
        "                    value=\"None\"\n",
        "                )\n",
        "\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to fly')\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                num_panels = gr.Slider(\n",
        "                    label=\"Number of Panels\",\n",
        "                    minimum=2,\n",
        "                    maximum=8,\n",
        "                    value=4,\n",
        "                    step=1\n",
        "                )\n",
        "\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\"Cartoon Style\", \"Realistic\", \"Anime\", \"Sketch\"],\n",
        "                    label=\"Art Style\",\n",
        "                    value=\"Cartoon Style\"\n",
        "                )\n",
        "\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"Generated Story\",\n",
        "                    placeholder=\"The generated story will appear here...\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                output_panels = gr.Gallery(\n",
        "                    label=\"Comic Panels\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"comic-panels\",\n",
        "                    height=200,\n",
        "                    columns=2\n",
        "                )\n",
        "\n",
        "                output_comic = gr.Image(\n",
        "                    label=\"Combined Comic Strip\",\n",
        "                    type=\"pil\"\n",
        "                )\n",
        "\n",
        "        # Fixed function to handle the generation and properly return outputs\n",
        "        def generate_comic_action(story_prompt, num_panels, art_style, use_template):\n",
        "            story_text, story_panels, panel_images, comic_strip = generate_comic(\n",
        "                story_prompt, num_panels, art_style=art_style, use_template=use_template\n",
        "            )\n",
        "            # Return the outputs directly rather than updating them\n",
        "            return story_text, panel_images if panel_images else [], comic_strip\n",
        "\n",
        "        generate_button.click(\n",
        "            generate_comic_action,\n",
        "            inputs=[story_prompt, num_panels, art_style, template_dropdown],\n",
        "            outputs=[output_text, output_panels, output_comic]\n",
        "        )\n",
        "\n",
        "    app.launch()\n",
        "\n",
        "# Mount Google Drive if in Colab environment\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Not running in Colab or drive already mounted\")\n",
        "\n",
        "# Call the function to create the UI\n",
        "if __name__ == \"__main__\":  # Fixed from 'name' to 'name_'\n",
        "    create_ui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "DyJsjY-bAHTn",
        "outputId": "aa908c68-45dd-4a94-a3d5-eedd71638c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3c317821eab3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStableDiffusionPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStableDiffusionXLPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1966\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1974\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1977\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderDecoderConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_LazyAutoMapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m from .configuration_auto import (\n\u001b[1;32m     40\u001b[0m     \u001b[0mCONFIG_MAPPING_NAMES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1974\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1977\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidate_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAssistantVocabTranslatorCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m from ..cache_utils import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/candidate_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_sklearn_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDynamicCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_is_arraylike_not_scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_numpy_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_preserve_dia_indices_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0m_NUMPY_NAMESPACE_NAMES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array_api_compat.numpy\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m from pandas.core.api import (\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m# dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mArrowDtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mvalue_counts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboolean\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBooleanDtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m from pandas.core.arrays.floating import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowExtensionArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from pandas.core.arrays.base import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mExtensionArray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mExtensionOpsMixin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mExtensionScalarOpsMixin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/arrow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.core.arrays.arrow.accessors import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mListAccessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mStructAccessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowExtensionArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/arrow/accessors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpa_version_under10p1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowDtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/compute.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from pyarrow._compute import (  # noqa\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mFunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mFunctionOptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/_compute.pyx\u001b[0m in \u001b[0;36minit pyarrow._compute\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should print True if GPU is available"
      ],
      "metadata": {
        "id": "WH9I30FyCKlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077ef7e2-5f8e-41ec-8b04-89619abf50e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python diffusers torch torchvision transformers accelerate numpy pillow opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8nI8rN5dr2I",
        "outputId": "a31da1ea-9153-4039-8e97-8a3c8e9a8889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.0)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.30.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp311-cp311-linux_x86_64.whl size=5959645 sha256=35d860fd4a9f3d6cd21678b000c8744a635b355417eb3baf90673cb4b4f6ee84\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/03/66/eb3810eafd55d921b2be32896d1f44313996982360663aa80b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, diskcache, nvidia-cusparse-cu12, nvidia-cudnn-cu12, llama-cpp-python, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.8 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvYNZ-dTd0ji",
        "outputId": "f3257215-6029-4c8a-cfc1-80a47ceb1724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.23.3-py3-none-any.whl (46.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python torch diffusers transformers accelerate safetensors matplotlib Pillow gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHWP-902e-Fj",
        "outputId": "b27962a9-07d8-497d-fbe9-593ed6d253ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.30.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLqdr-L_fGXE",
        "outputId": "b12972b0-ee83-404e-f0cf-d74837ae289d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Try loading a fallback model if the fine-tuned one fails\n",
        "        try:\n",
        "            print(\"Attempting to load fallback model gpt2...\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = model.to(device)\n",
        "            print(f\"Fallback model loaded successfully and moved to {device}\")\n",
        "            return model, tokenizer\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading fallback model: {e2}\")\n",
        "            return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_story(model, tokenizer, prompt, max_length=400, retries=3):\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    # Increase repetition penalty and adjust parameters for better generation\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "            device = next(model.parameters()).device\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Use different generation parameters for each retry attempt\n",
        "            temperature = 0.7 + (attempt * 0.1)  # Gradually increase temperature with each retry\n",
        "            repetition_penalty = 1.2 + (attempt * 0.1)  # Gradually increase repetition penalty\n",
        "\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=3,  # Increased from 2\n",
        "                top_k=50,  # Increased from 40\n",
        "                top_p=0.92,  # Adjusted from 0.9\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=repetition_penalty\n",
        "            )\n",
        "\n",
        "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Clean up the generated text to focus on the actual story\n",
        "            if story_prompt in generated_text:\n",
        "                generated_text = generated_text.replace(story_prompt, \"\").strip()\n",
        "\n",
        "            # Check if we have a reasonable story\n",
        "            if len(generated_text) > 100 and \".\" in generated_text:\n",
        "                print(f\"Successfully generated story on attempt {attempt+1}\")\n",
        "                return generated_text\n",
        "            else:\n",
        "                print(f\"Attempt {attempt+1} produced insufficient content ({len(generated_text)} chars). Retrying...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in story generation attempt {attempt+1}: {e}\")\n",
        "\n",
        "    # If all retries fail, generate a more varied story using a different approach\n",
        "    print(\"All generation attempts failed. Using alternative generation method...\")\n",
        "    return generate_alternative_story(prompt)\n",
        "\n",
        "def generate_alternative_story(prompt):\n",
        "    \"\"\"\n",
        "    Generate a more varied story when the model-based generation fails.\n",
        "    This creates a more dynamic story than the previous structured approach.\n",
        "    \"\"\"\n",
        "    character = extract_character_name(prompt) or \"the little hero\"\n",
        "    topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "    # Create a list of possible story templates\n",
        "    story_templates = [\n",
        "        f\"\"\"Once upon a time, there was {character} who lived in a small village.\n",
        "{character} loved {topic} more than anything else.\n",
        "\n",
        "One day, {character} discovered something magical while playing outside. It was a tiny glowing ball that could float in the air.\n",
        "\n",
        "\"What's this?\" wondered {character}. The ball started to move, and {character} followed it into the forest.\n",
        "\n",
        "The ball led {character} to a hidden garden where other children were playing with similar magical balls. They welcomed {character} and taught them new games.\n",
        "\n",
        "{character} had so much fun that day! When it was time to go home, one of the children gave {character} a magical ball of their own.\n",
        "\n",
        "From that day on, {character} visited the hidden garden often and made many new friends. They all loved {topic} together and had wonderful adventures.\"\"\",\n",
        "\n",
        "        f\"\"\"{character} woke up early one morning with a big smile. Today was the day for {topic}!\n",
        "\n",
        "{character} packed a small bag with snacks and a water bottle. \"I'm ready for my adventure!\" {character} said.\n",
        "\n",
        "Outside, {character} met a friendly cat. \"Hello, cat! Would you like to join me for {topic}?\" The cat meowed and followed along.\n",
        "\n",
        "As they walked through the park, they found a lost teddy bear. \"We should find who this belongs to,\" said {character}.\n",
        "\n",
        "They asked everyone they met about the teddy bear. Finally, they met a little girl who was crying. \"My teddy!\" she said happily when she saw it.\n",
        "\n",
        "\"Thank you for helping me find my teddy,\" the girl smiled. \"Would you like to play with me?\"\n",
        "\n",
        "{character}, the cat, and the girl spent the whole day playing and having fun with {topic}. It was the best day ever!\"\"\",\n",
        "\n",
        "        f\"\"\"The sun was shining brightly when {character} decided it was the perfect day for {topic}.\n",
        "\n",
        "\"I've never tried {topic} before,\" {character} thought, \"but today I will be brave!\"\n",
        "\n",
        "{character} began slowly, taking small steps. It wasn't easy at first, and {character} almost gave up.\n",
        "\n",
        "\"Don't worry, you can do it!\" said a friendly voice. {character} looked up and saw a smiling butterfly.\n",
        "\n",
        "With the butterfly's encouragement, {character} tried again. This time, {character} did much better!\n",
        "\n",
        "Soon, {character} was having so much fun with {topic}. \"Thank you for believing in me,\" {character} said to the butterfly.\n",
        "\n",
        "The butterfly fluttered around happily. \"Sometimes we just need a friend to help us be brave,\" it replied.\n",
        "\n",
        "{character} went home that evening with a happy heart, excited to try {topic} again tomorrow.\"\"\"\n",
        "    ]\n",
        "\n",
        "    import random\n",
        "    return random.choice(story_templates)\n",
        "\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        if len(paragraphs) > num_panels:\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    sentences = []\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]\n",
        "\n",
        "    if len(sentences) < num_panels:\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]\n",
        "        except:\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "def get_template_data(template_name):\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)],\n",
        "            \"story\": \"\"\"The Dark Knight watches over Gotham City from high above. Tonight is like any other night for Batman - a city full of shadows that need his protection.\n",
        "\n",
        "As the Bat-Signal illuminates the cloudy sky, Batman's eyes narrow with determination. The signal isn't just a call for help - it's a reminder of the oath he took to protect the innocent.\n",
        "\n",
        "Batman spots a group of criminals attempting to rob a local shop. He leaps from the rooftop, his cape billowing around him like massive wings. The criminals look up in terror as his silhouette blots out the moonlight.\n",
        "\n",
        "With swift, calculated movements, Batman takes down the criminals one by one. Another night in Gotham, another battle won. But Batman knows his work is never truly done. As long as there is crime in Gotham, he will be there to fight it.\"\"\"\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)],\n",
        "            \"story\": \"\"\"Peter Parker sits perched on the edge of a towering skyscraper, overlooking the bustling streets of New York City. His uncle Ben's words echo in his mind as he surveys the city he's sworn to protect.\n",
        "\n",
        "With a flick of his wrist, Spider-Man launches himself off the building, shooting a web that attaches to a nearby structure. He swings gracefully through the concrete canyons of Manhattan, the wind rushing past his mask as pedestrians below point and cheer.\n",
        "\n",
        "Suddenly, his spider-sense alerts him to danger. A bank robbery is in progress just a few blocks away. Spider-Man changes direction mid-swing, heading straight for the trouble. As he arrives, he sees masked robbers threatening innocent civilians.\n",
        "\n",
        "With swift movements and well-placed webs, Spider-Man subdues the criminals and ensures everyone is safe. A young child approaches him with wide eyes of admiration. Spider-Man kneels down to their level, reminding them that anyone can be a hero if they choose to help others.\"\"\"\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)],\n",
        "            \"story\": \"\"\"The sun rises over Themyscira, casting a golden glow on the island paradise. Diana stands proudly on a cliff overlooking the ocean, her armor gleaming in the morning light, ready to face whatever challenges the day may bring.\n",
        "\n",
        "News reaches the Amazon princess of danger in the world of men. Without hesitation, Diana leaves her homeland behind. In the midst of chaos, Wonder Woman stands firm, her bracelets deflecting a barrage of bullets meant to harm innocent bystanders.\n",
        "\n",
        "Ancient mythological creatures, released by a power-hungry sorcerer, terrorize a small village. Wonder Woman meets them in battle, her sword and shield moving with incredible speed and precision. Her training as an Amazon warrior guides each strike.\n",
        "\n",
        "With the creatures defeated, Wonder Woman confronts the sorcerer. She wraps her golden Lasso of Truth around him, the mystical rope glowing with divine energy. As the magic of the lasso compels him to speak honestly, the truth behind his actions is revealed, and justice can finally be served.\"\"\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        # Use the pre-written story from the template data\n",
        "        story_text = template_data[\"story\"]\n",
        "        story_panels = prompts\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # If story generation still produces insufficient content after multiple attempts\n",
        "        # and with different parameters, try generating with a completely different approach\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Using alternative generation approach.\")\n",
        "            story_text = generate_alternative_story(story_prompt)\n",
        "            print(\"Alternative story generated:\")\n",
        "            print(story_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in model-based story generation: {e}\")\n",
        "        print(\"Using alternative story generation approach...\")\n",
        "        story_text = generate_alternative_story(story_prompt)\n",
        "\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Make the image prompts unique for each panel based on the actual story content\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                position = (256, 100 + (i % 2) * 50)\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),\n",
        "                    label=\"Choose Template (optional)\",\n",
        "                    value=\"None\"\n",
        "                )\n",
        "\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to fly')\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                num_panels = gr.Slider(\n",
        "                    label=\"Number of Panels\",\n",
        "                    minimum=2,\n",
        "                    maximum=8,\n",
        "                    value=4,\n",
        "                    step=1\n",
        "                )\n",
        "\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\"Cartoon Style\", \"Realistic\", \"Anime\", \"Sketch\"],\n",
        "                    label=\"Art Style\",\n",
        "                    value=\"Cartoon Style\"\n",
        "                )\n",
        "\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"Generated Story\",\n",
        "                    placeholder=\"The generated story will appear here...\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                output_panels = gr.Gallery(\n",
        "                    label=\"Comic Panels\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"comic-panels\",\n",
        "                    height=200,\n",
        "                    columns=2\n",
        "                )\n",
        "\n",
        "                output_comic = gr.Image(\n",
        "                    label=\"Combined Comic Strip\",\n",
        "                    type=\"pil\"\n",
        "                )\n",
        "\n",
        "        # Fixed function to handle the generation and properly return outputs\n",
        "        def generate_comic_action(story_prompt, num_panels, art_style, use_template):\n",
        "            story_text, story_panels, panel_images, comic_strip = generate_comic(\n",
        "                story_prompt, num_panels, art_style=art_style, use_template=use_template\n",
        "            )\n",
        "            # Return the outputs directly rather than updating them\n",
        "            return story_text, panel_images if panel_images else [], comic_strip\n",
        "\n",
        "        generate_button.click(\n",
        "            generate_comic_action,\n",
        "            inputs=[story_prompt, num_panels, art_style, template_dropdown],\n",
        "            outputs=[output_text, output_panels, output_comic]\n",
        "        )\n",
        "\n",
        "    app.launch()\n",
        "\n",
        "# Mount Google Drive if in Colab environment\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Not running in Colab or drive already mounted\")\n",
        "\n",
        "# Call the function to create the UI\n",
        "if __name__ == \"__main__\":  # Fixed from 'name' to 'name_'\n",
        "    create_ui()"
      ],
      "metadata": {
        "id": "DhQJWb88gkPK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "b3f0338d-3e08-4dfd-b4fe-b0a97c8a61eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gradio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3c317821eab3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStableDiffusionPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStableDiffusionXLPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "rerrPekzKr6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f724a468-cc97-456e-8cd0-a2c6002f8373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.26.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.9.0 (from gradio)\n",
            "  Downloading gradio_client-1.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.26.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.9.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.26.0 gradio-client-1.9.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Try loading a fallback model if the fine-tuned one fails\n",
        "        try:\n",
        "            print(\"Attempting to load fallback model gpt2...\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = model.to(device)\n",
        "            print(f\"Fallback model loaded successfully and moved to {device}\")\n",
        "            return model, tokenizer\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading fallback model: {e2}\")\n",
        "            return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_story(model, tokenizer, prompt, max_length=400, retries=3):\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    # Increase repetition penalty and adjust parameters for better generation\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "            device = next(model.parameters()).device\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Use different generation parameters for each retry attempt\n",
        "            temperature = 0.7 + (attempt * 0.1)  # Gradually increase temperature with each retry\n",
        "            repetition_penalty = 1.2 + (attempt * 0.1)  # Gradually increase repetition penalty\n",
        "\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=3,  # Increased from 2\n",
        "                top_k=50,  # Increased from 40\n",
        "                top_p=0.92,  # Adjusted from 0.9\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=repetition_penalty\n",
        "            )\n",
        "\n",
        "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Clean up the generated text to focus on the actual story\n",
        "            if story_prompt in generated_text:\n",
        "                generated_text = generated_text.replace(story_prompt, \"\").strip()\n",
        "\n",
        "            # Check if we have a reasonable story\n",
        "            if len(generated_text) > 100 and \".\" in generated_text:\n",
        "                print(f\"Successfully generated story on attempt {attempt+1}\")\n",
        "                return generated_text\n",
        "            else:\n",
        "                print(f\"Attempt {attempt+1} produced insufficient content ({len(generated_text)} chars). Retrying...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in story generation attempt {attempt+1}: {e}\")\n",
        "\n",
        "    # If all retries fail, generate a more varied story using a different approach\n",
        "    print(\"All generation attempts failed. Using alternative generation method...\")\n",
        "    return generate_alternative_story(prompt)\n",
        "\n",
        "def generate_alternative_story(prompt):\n",
        "    \"\"\"\n",
        "    Generate a more varied story when the model-based generation fails.\n",
        "    This creates a more dynamic story than the previous structured approach.\n",
        "    \"\"\"\n",
        "    character = extract_character_name(prompt) or \"the little hero\"\n",
        "    topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "    # Create a list of possible story templates\n",
        "    story_templates = [\n",
        "        f\"\"\"Once upon a time, there was {character} who lived in a small village.\n",
        "{character} loved {topic} more than anything else.\n",
        "\n",
        "One day, {character} discovered something magical while playing outside. It was a tiny glowing ball that could float in the air.\n",
        "\n",
        "\"What's this?\" wondered {character}. The ball started to move, and {character} followed it into the forest.\n",
        "\n",
        "The ball led {character} to a hidden garden where other children were playing with similar magical balls. They welcomed {character} and taught them new games.\n",
        "\n",
        "{character} had so much fun that day! When it was time to go home, one of the children gave {character} a magical ball of their own.\n",
        "\n",
        "From that day on, {character} visited the hidden garden often and made many new friends. They all loved {topic} together and had wonderful adventures.\"\"\",\n",
        "\n",
        "        f\"\"\"{character} woke up early one morning with a big smile. Today was the day for {topic}!\n",
        "\n",
        "{character} packed a small bag with snacks and a water bottle. \"I'm ready for my adventure!\" {character} said.\n",
        "\n",
        "Outside, {character} met a friendly cat. \"Hello, cat! Would you like to join me for {topic}?\" The cat meowed and followed along.\n",
        "\n",
        "As they walked through the park, they found a lost teddy bear. \"We should find who this belongs to,\" said {character}.\n",
        "\n",
        "They asked everyone they met about the teddy bear. Finally, they met a little girl who was crying. \"My teddy!\" she said happily when she saw it.\n",
        "\n",
        "\"Thank you for helping me find my teddy,\" the girl smiled. \"Would you like to play with me?\"\n",
        "\n",
        "{character}, the cat, and the girl spent the whole day playing and having fun with {topic}. It was the best day ever!\"\"\",\n",
        "\n",
        "        f\"\"\"The sun was shining brightly when {character} decided it was the perfect day for {topic}.\n",
        "\n",
        "\"I've never tried {topic} before,\" {character} thought, \"but today I will be brave!\"\n",
        "\n",
        "{character} began slowly, taking small steps. It wasn't easy at first, and {character} almost gave up.\n",
        "\n",
        "\"Don't worry, you can do it!\" said a friendly voice. {character} looked up and saw a smiling butterfly.\n",
        "\n",
        "With the butterfly's encouragement, {character} tried again. This time, {character} did much better!\n",
        "\n",
        "Soon, {character} was having so much fun with {topic}. \"Thank you for believing in me,\" {character} said to the butterfly.\n",
        "\n",
        "The butterfly fluttered around happily. \"Sometimes we just need a friend to help us be brave,\" it replied.\n",
        "\n",
        "{character} went home that evening with a happy heart, excited to try {topic} again tomorrow.\"\"\"\n",
        "    ]\n",
        "\n",
        "    import random\n",
        "    return random.choice(story_templates)\n",
        "\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        if len(paragraphs) > num_panels:\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    sentences = []\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]\n",
        "\n",
        "    if len(sentences) < num_panels:\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]\n",
        "        except:\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "def get_template_data(template_name):\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)],\n",
        "            \"story\": \"\"\"The Dark Knight watches over Gotham City from high above. Tonight is like any other night for Batman - a city full of shadows that need his protection.\n",
        "\n",
        "As the Bat-Signal illuminates the cloudy sky, Batman's eyes narrow with determination. The signal isn't just a call for help - it's a reminder of the oath he took to protect the innocent.\n",
        "\n",
        "Batman spots a group of criminals attempting to rob a local shop. He leaps from the rooftop, his cape billowing around him like massive wings. The criminals look up in terror as his silhouette blots out the moonlight.\n",
        "\n",
        "With swift, calculated movements, Batman takes down the criminals one by one. Another night in Gotham, another battle won. But Batman knows his work is never truly done. As long as there is crime in Gotham, he will be there to fight it.\"\"\"\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)],\n",
        "            \"story\": \"\"\"Peter Parker sits perched on the edge of a towering skyscraper, overlooking the bustling streets of New York City. His uncle Ben's words echo in his mind as he surveys the city he's sworn to protect.\n",
        "\n",
        "With a flick of his wrist, Spider-Man launches himself off the building, shooting a web that attaches to a nearby structure. He swings gracefully through the concrete canyons of Manhattan, the wind rushing past his mask as pedestrians below point and cheer.\n",
        "\n",
        "Suddenly, his spider-sense alerts him to danger. A bank robbery is in progress just a few blocks away. Spider-Man changes direction mid-swing, heading straight for the trouble. As he arrives, he sees masked robbers threatening innocent civilians.\n",
        "\n",
        "With swift movements and well-placed webs, Spider-Man subdues the criminals and ensures everyone is safe. A young child approaches him with wide eyes of admiration. Spider-Man kneels down to their level, reminding them that anyone can be a hero if they choose to help others.\"\"\"\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)],\n",
        "            \"story\": \"\"\"The sun rises over Themyscira, casting a golden glow on the island paradise. Diana stands proudly on a cliff overlooking the ocean, her armor gleaming in the morning light, ready to face whatever challenges the day may bring.\n",
        "\n",
        "News reaches the Amazon princess of danger in the world of men. Without hesitation, Diana leaves her homeland behind. In the midst of chaos, Wonder Woman stands firm, her bracelets deflecting a barrage of bullets meant to harm innocent bystanders.\n",
        "\n",
        "Ancient mythological creatures, released by a power-hungry sorcerer, terrorize a small village. Wonder Woman meets them in battle, her sword and shield moving with incredible speed and precision. Her training as an Amazon warrior guides each strike.\n",
        "\n",
        "With the creatures defeated, Wonder Woman confronts the sorcerer. She wraps her golden Lasso of Truth around him, the mystical rope glowing with divine energy. As the magic of the lasso compels him to speak honestly, the truth behind his actions is revealed, and justice can finally be served.\"\"\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        # Use the pre-written story from the template data\n",
        "        story_text = template_data[\"story\"]\n",
        "        story_panels = prompts\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # If story generation still produces insufficient content after multiple attempts\n",
        "        # and with different parameters, try generating with a completely different approach\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Using alternative generation approach.\")\n",
        "            story_text = generate_alternative_story(story_prompt)\n",
        "            print(\"Alternative story generated:\")\n",
        "            print(story_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in model-based story generation: {e}\")\n",
        "        print(\"Using alternative story generation approach...\")\n",
        "        story_text = generate_alternative_story(story_prompt)\n",
        "\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Make the image prompts unique for each panel based on the actual story content\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                position = (256, 100 + (i % 2) * 50)\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),\n",
        "                    label=\"Choose Template (optional)\",\n",
        "                    value=\"None\"\n",
        "                )\n",
        "\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to fly')\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                num_panels = gr.Slider(\n",
        "                    label=\"Number of Panels\",\n",
        "                    minimum=2,\n",
        "                    maximum=8,\n",
        "                    value=4,\n",
        "                    step=1\n",
        "                )\n",
        "\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\"Cartoon Style\", \"Realistic\", \"Anime\", \"Sketch\"],\n",
        "                    label=\"Art Style\",\n",
        "                    value=\"Cartoon Style\"\n",
        "                )\n",
        "\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"Generated Story\",\n",
        "                    placeholder=\"The generated story will appear here...\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                output_panels = gr.Gallery(\n",
        "                    label=\"Comic Panels\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"comic-panels\",\n",
        "                    height=200,\n",
        "                    columns=2\n",
        "                )\n",
        "\n",
        "                output_comic = gr.Image(\n",
        "                    label=\"Combined Comic Strip\",\n",
        "                    type=\"pil\"\n",
        "                )\n",
        "\n",
        "        # Fixed function to handle the generation and properly return outputs\n",
        "        def generate_comic_action(story_prompt, num_panels, art_style, use_template):\n",
        "            story_text, story_panels, panel_images, comic_strip = generate_comic(\n",
        "                story_prompt, num_panels, art_style=art_style, use_template=use_template\n",
        "            )\n",
        "            # Return the outputs directly rather than updating them\n",
        "            return story_text, panel_images if panel_images else [], comic_strip\n",
        "\n",
        "        generate_button.click(\n",
        "            generate_comic_action,\n",
        "            inputs=[story_prompt, num_panels, art_style, template_dropdown],\n",
        "            outputs=[output_text, output_panels, output_comic]\n",
        "        )\n",
        "\n",
        "    app.launch()\n",
        "\n",
        "# Mount Google Drive if in Colab environment\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Not running in Colab or drive already mounted\")\n",
        "\n",
        "# Call the function to create the UI\n",
        "if __name__ == \"__main__\":  # Fixed from 'name' to 'name_'\n",
        "    create_ui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "Q0PsSBPTTMZs",
        "outputId": "6210ed10-11d5-4885-963c-37a5f4ab5b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7d3227aabc390bc96c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7d3227aabc390bc96c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pDY59VKlVuYc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}